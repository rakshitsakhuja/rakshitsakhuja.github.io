[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Rakshit Sakhuja, passionate about Machine Learning and AI technologies."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "I’m Rakshit Sakhuja, passionate about Machine Learning and AI technologies."
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About",
    "section": "About This Blog",
    "text": "About This Blog\nThis blog is powered by Quarto and contains my notes, experiments, and insights on:\n\nMachine Learning\nDeep Learning\n\nAzure ML Services\nData Science\nAI Research\n\nOne of the beautiful things about Machine Learning is you are not going to remember a lot of details if you are not practicing it - hence this blog serves as my external memory!"
  },
  {
    "objectID": "about.html#connect",
    "href": "about.html#connect",
    "title": "About",
    "section": "Connect",
    "text": "Connect\n\nGitHub: @rakshitsakhuja\nTwitter: @rakki_99"
  },
  {
    "objectID": "posts/2025-08-08-elasticsearch/index.html",
    "href": "posts/2025-08-08-elasticsearch/index.html",
    "title": "ElasticSearch - Create and Delete operations in Python",
    "section": "",
    "text": "First, install the ElasticSearch Python client:\n!pip install elasticsearch"
  },
  {
    "objectID": "posts/2025-08-08-elasticsearch/index.html#installation-and-setup",
    "href": "posts/2025-08-08-elasticsearch/index.html#installation-and-setup",
    "title": "ElasticSearch - Create and Delete operations in Python",
    "section": "",
    "text": "First, install the ElasticSearch Python client:\n!pip install elasticsearch"
  },
  {
    "objectID": "posts/2025-08-08-elasticsearch/index.html#import-required-libraries",
    "href": "posts/2025-08-08-elasticsearch/index.html#import-required-libraries",
    "title": "ElasticSearch - Create and Delete operations in Python",
    "section": "Import Required Libraries",
    "text": "Import Required Libraries\ntry:\n    import os\n    import sys\n    import elasticsearch\n    from elasticsearch import Elasticsearch\n    import pandas as pd\n    print(\"All Modules Loaded ! \")\nexcept Exception as e:\n    print(\"Some Modules are Missing {}\".format(e))"
  },
  {
    "objectID": "posts/2025-08-08-elasticsearch/index.html#connecting-to-elasticsearch",
    "href": "posts/2025-08-08-elasticsearch/index.html#connecting-to-elasticsearch",
    "title": "ElasticSearch - Create and Delete operations in Python",
    "section": "Connecting to ElasticSearch",
    "text": "Connecting to ElasticSearch\ndef connect_elasticsearch():\n    es = None\n    es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n    if es.ping():\n        print('Connected ')\n    else:\n        print('Please Check!..not connected!')\n    return es\n\nes = connect_elasticsearch()"
  },
  {
    "objectID": "posts/2025-08-08-elasticsearch/index.html#creating-indices",
    "href": "posts/2025-08-08-elasticsearch/index.html#creating-indices",
    "title": "ElasticSearch - Create and Delete operations in Python",
    "section": "Creating Indices",
    "text": "Creating Indices\nCreate new indices in ElasticSearch:\nes.indices.create(index='test-index', ignore=400)\nes.indices.create(index='test-index1', ignore=400)\nThe ignore=400 parameter ignores the error if the index already exists."
  },
  {
    "objectID": "posts/2025-08-08-elasticsearch/index.html#listing-all-indices",
    "href": "posts/2025-08-08-elasticsearch/index.html#listing-all-indices",
    "title": "ElasticSearch - Create and Delete operations in Python",
    "section": "Listing All Indices",
    "text": "Listing All Indices\n\nMethod 1: Get all indices including system indices\nres = es.indices.get_alias(\"*\")\nfor Name in res:\n    print(Name)\n\n\nMethod 2: Get indices as dict keys\nindices = es.indices.get_alias().keys()\nfor Name in indices:\n    print(Name)\n\n\nMethod 3: Filter specific indices\nfor Name in [i for i in es.indices.get_alias().keys() if i in ['test-index1','test-index']]:\n    print(\"Deleted {} \".format(Name))\n    es.indices.delete(index=Name, ignore=[400, 404])"
  },
  {
    "objectID": "posts/2025-08-08-elasticsearch/index.html#working-with-documents",
    "href": "posts/2025-08-08-elasticsearch/index.html#working-with-documents",
    "title": "ElasticSearch - Create and Delete operations in Python",
    "section": "Working with Documents",
    "text": "Working with Documents\n\nSample Data\nCreate sample documents to index:\ne1 = {\n    \"first_name\": \"Soumil\",\n    \"last_name\": \"Shah\",\n    \"age\": 24,\n    \"about\": \"Full stack Software Developers \",\n    \"interests\": ['Youtube','music'],\n}\n\ne2 = {\n    \"first_name\": \"nitin\",\n    \"last_name\": \"Shah\",\n    \"age\": 58,\n    \"about\": \"Soumil father \",\n    \"interests\": ['Stock','Relax'],\n}\n\n\nCreating Documents\nCreate an index and add documents:\nes.indices.create(index='person', ignore=400)\n\nres1 = es.index(index='person', doc_type='people', body=e1)\nres2 = es.index(index='person', doc_type='people', body=e2)\n\nprint(\"RES1 : {}\".format(res1))\nprint(\"RES2 : {}\".format(res2))\nNote: The doc_type parameter is deprecated in newer versions of ElasticSearch. Use typeless endpoints instead."
  },
  {
    "objectID": "posts/2025-08-08-elasticsearch/index.html#searching-documents",
    "href": "posts/2025-08-08-elasticsearch/index.html#searching-documents",
    "title": "ElasticSearch - Create and Delete operations in Python",
    "section": "Searching Documents",
    "text": "Searching Documents\n\nMatch All Query\nSearch for all documents in an index:\nquery = {\"query\": {\n        \"match_all\": {}\n    }}\n\nres = es.search(index=\"person\", body=query, size=1000)\n\n\nDisplay Results as DataFrame\nConvert search results to a pandas DataFrame:\npd.DataFrame(list(res['hits']['hits']))\nThis will show a table with columns: - _index: The index name - _type: The document type (deprecated) - _id: Unique document ID - _score: Relevance score - _source: The actual document data"
  },
  {
    "objectID": "posts/2025-08-08-elasticsearch/index.html#key-elasticsearch-concepts",
    "href": "posts/2025-08-08-elasticsearch/index.html#key-elasticsearch-concepts",
    "title": "ElasticSearch - Create and Delete operations in Python",
    "section": "Key ElasticSearch Concepts",
    "text": "Key ElasticSearch Concepts\n\nIndex: Similar to a database in RDBMS\nDocument: Similar to a row in a table\nField: Similar to a column in a table\nMapping: Similar to a schema definition\nQuery DSL: ElasticSearch’s query language"
  },
  {
    "objectID": "posts/2025-08-08-elasticsearch/index.html#common-operations-summary",
    "href": "posts/2025-08-08-elasticsearch/index.html#common-operations-summary",
    "title": "ElasticSearch - Create and Delete operations in Python",
    "section": "Common Operations Summary",
    "text": "Common Operations Summary\n\nCreate Index: es.indices.create(index='name', ignore=400)\nDelete Index: es.indices.delete(index='name', ignore=[400, 404])\nList Indices: es.indices.get_alias().keys()\nAdd Document: es.index(index='name', body=document)\nSearch Documents: es.search(index='name', body=query)\nCheck Connection: es.ping()"
  },
  {
    "objectID": "posts/2025-08-08-elasticsearch/index.html#error-handling",
    "href": "posts/2025-08-08-elasticsearch/index.html#error-handling",
    "title": "ElasticSearch - Create and Delete operations in Python",
    "section": "Error Handling",
    "text": "Error Handling\nAlways use the ignore parameter to handle common HTTP errors: - 400: Bad Request (e.g., index already exists) - 404: Not Found (e.g., index doesn’t exist)\nThis tutorial covers the basic CRUD operations in ElasticSearch using Python, providing a foundation for more complex search and analytics operations."
  },
  {
    "objectID": "posts/2025-01-09-quantization-distillation/index.html",
    "href": "posts/2025-01-09-quantization-distillation/index.html",
    "title": "Model Optimization Techniques: A Deep Dive into Quantization and Knowledge Distillation",
    "section": "",
    "text": "In the era of large language models and complex neural networks, the gap between model capability and deployment constraints has never been wider. While a 70B parameter model might achieve state-of-the-art results, deploying it in production often requires significant optimization. This comprehensive guide explores two of the most effective model compression techniques: quantization and knowledge distillation.\n\n\nLet me start with a story from my experience at Mediacorp. We had built an excellent recommendation model using a large transformer architecture that achieved 40% CTR improvement in our offline tests. However, when we tried to deploy it to serve millions of users across web, mobile, and TV platforms, we faced several challenges:\n\nLatency: The model took 300ms per inference, far too slow for real-time recommendations\nMemory: Each model instance required 8GB of GPU memory, limiting our serving capacity\nCost: Running the model at scale would have cost us 10x our infrastructure budget\n\nThis is where model optimization techniques became not just nice-to-have, but absolutely critical for production deployment.\n\n\n\n\n\nQuantization is the process of reducing the numerical precision of model weights and activations. Instead of using 32-bit floating point numbers (FP32), we can use 16-bit (FP16), 8-bit (INT8), or even 4-bit representations while maintaining acceptable model performance.\nThink of quantization like compressing a high-resolution image - you lose some detail, but the essential information remains, and the file size becomes much more manageable.\n\n\n\n\n\nThis is the simplest approach where we quantize a pre-trained model without any additional training.\nimport torch\nimport torch.quantization as quant\n\n# Example: Post-training quantization with PyTorch\ndef quantize_model_ptq(model, calibration_dataloader):\n    \"\"\"\n    Apply post-training quantization to a PyTorch model\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n    \n    # Specify quantization configuration\n    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n    \n    # Prepare model for quantization\n    model_prepared = torch.quantization.prepare(model)\n    \n    # Calibrate with representative data\n    with torch.no_grad():\n        for batch in calibration_dataloader:\n            model_prepared(batch)\n    \n    # Convert to quantized model\n    quantized_model = torch.quantization.convert(model_prepared)\n    \n    return quantized_model\n\n# Real-world usage example\ndef benchmark_quantization():\n    \"\"\"\n    Compare original vs quantized model performance\n    \"\"\"\n    original_model = YourTransformerModel()\n    quantized_model = quantize_model_ptq(original_model, calibration_loader)\n    \n    # Memory comparison\n    original_size = sum(p.numel() * p.element_size() for p in original_model.parameters())\n    quantized_size = sum(p.numel() * p.element_size() for p in quantized_model.parameters())\n    \n    print(f\"Original model size: {original_size / 1e6:.2f} MB\")\n    print(f\"Quantized model size: {quantized_size / 1e6:.2f} MB\")\n    print(f\"Compression ratio: {original_size / quantized_size:.2f}x\")\n    \n    # Latency comparison\n    import time\n    \n    # Warm up\n    dummy_input = torch.randn(1, 512, 768)\n    for _ in range(10):\n        _ = original_model(dummy_input)\n        _ = quantized_model(dummy_input)\n    \n    # Benchmark\n    start_time = time.time()\n    for _ in range(100):\n        _ = original_model(dummy_input)\n    original_time = time.time() - start_time\n    \n    start_time = time.time()\n    for _ in range(100):\n        _ = quantized_model(dummy_input)\n    quantized_time = time.time() - start_time\n    \n    print(f\"Original inference time: {original_time/100*1000:.2f} ms\")\n    print(f\"Quantized inference time: {quantized_time/100*1000:.2f} ms\")\n    print(f\"Speedup: {original_time/quantized_time:.2f}x\")\n\n\n\nQAT simulates quantization effects during training, allowing the model to adapt to reduced precision.\nimport torch.nn as nn\nimport torch.quantization as quant\n\nclass QuantizedTransformerBlock(nn.Module):\n    \"\"\"\n    A transformer block prepared for quantization-aware training\n    \"\"\"\n    def __init__(self, d_model, nhead, dim_feedforward):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(d_model, nhead)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n        self.feedforward = nn.Sequential(\n            nn.Linear(d_model, dim_feedforward),\n            nn.ReLU(),\n            nn.Linear(dim_feedforward, d_model)\n        )\n        \n        # Add quantization stubs\n        self.quant = quant.QuantStub()\n        self.dequant = quant.DeQuantStub()\n        \n    def forward(self, x):\n        x = self.quant(x)\n        \n        # Attention block with residual connection\n        attn_out, _ = self.attention(x, x, x)\n        x = self.norm1(x + attn_out)\n        \n        # Feedforward block with residual connection\n        ff_out = self.feedforward(x)\n        x = self.norm2(x + ff_out)\n        \n        x = self.dequant(x)\n        return x\n\ndef train_with_qat(model, train_loader, num_epochs=10):\n    \"\"\"\n    Training loop for quantization-aware training\n    \"\"\"\n    # Configure quantization settings\n    model.qconfig = quant.get_default_qat_qconfig('fbgemm')\n    \n    # Prepare model for QAT\n    model_prepared = quant.prepare_qat(model, inplace=False)\n    \n    # Training loop\n    optimizer = torch.optim.AdamW(model_prepared.parameters(), lr=1e-4)\n    criterion = nn.CrossEntropyLoss()\n    \n    model_prepared.train()\n    for epoch in range(num_epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model_prepared(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            \n            if batch_idx % 100 == 0:\n                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n    \n    # Convert to quantized model for deployment\n    model_prepared.eval()\n    quantized_model = quant.convert(model_prepared, inplace=False)\n    \n    return quantized_model\n\n\n\n\n\n\nPerfect for models with varying input sizes, like NLP applications:\ndef apply_dynamic_quantization(model):\n    \"\"\"\n    Apply dynamic quantization - weights are quantized, \n    activations are quantized dynamically during inference\n    \"\"\"\n    quantized_model = torch.quantization.quantize_dynamic(\n        model, \n        {nn.Linear, nn.LSTM, nn.GRU}, \n        dtype=torch.qint8\n    )\n    return quantized_model\n\n# Example usage with a BERT-style model\nclass OptimizedBERT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # Your BERT implementation here\n        pass\n    \n    def optimize_for_production(self):\n        \"\"\"\n        Apply multiple optimization techniques\n        \"\"\"\n        # First, apply dynamic quantization\n        self = torch.quantization.quantize_dynamic(\n            self, \n            {nn.Linear}, \n            dtype=torch.qint8\n        )\n        \n        # Then, fuse operations for better performance\n        self = torch.jit.script(self)\n        \n        return self\n\n\n\nUsing different precisions for different parts of the model:\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef train_with_mixed_precision(model, train_loader, num_epochs=10):\n    \"\"\"\n    Training with Automatic Mixed Precision for better performance\n    \"\"\"\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n    scaler = GradScaler()\n    criterion = nn.CrossEntropyLoss()\n    \n    model.train()\n    for epoch in range(num_epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            \n            # Forward pass with autocast\n            with autocast():\n                output = model(data)\n                loss = criterion(output, target)\n            \n            # Backward pass with gradient scaling\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            if batch_idx % 100 == 0:\n                print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n\n\n\n\n\n\n\nKnowledge distillation is like having an experienced mentor teach a junior developer. The mentor (teacher model) has years of experience and deep understanding, while the junior (student model) is eager to learn but needs guidance to develop similar intuition quickly.\nIn ML terms, we train a smaller, faster “student” model to mimic the behavior of a larger, more accurate “teacher” model.\n\n\n\n\n\nThe student learns from the teacher’s final outputs:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DistillationLoss(nn.Module):\n    \"\"\"\n    Combined loss for knowledge distillation\n    \"\"\"\n    def __init__(self, alpha=0.3, temperature=4.0):\n        super().__init__()\n        self.alpha = alpha  # Weight for distillation loss\n        self.temperature = temperature  # Temperature for softmax\n        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n        self.ce_loss = nn.CrossEntropyLoss()\n        \n    def forward(self, student_logits, teacher_logits, target):\n        # Distillation loss - student learns from teacher's soft predictions\n        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)\n        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)\n        distillation_loss = self.kl_div(student_log_probs, teacher_probs)\n        \n        # Standard classification loss - student learns from true labels\n        classification_loss = self.ce_loss(student_logits, target)\n        \n        # Combined loss\n        total_loss = (\n            self.alpha * distillation_loss * (self.temperature ** 2) +\n            (1 - self.alpha) * classification_loss\n        )\n        \n        return total_loss, distillation_loss, classification_loss\n\ndef train_student_model(teacher_model, student_model, train_loader, num_epochs=20):\n    \"\"\"\n    Train student model using knowledge distillation\n    \"\"\"\n    teacher_model.eval()  # Teacher is frozen\n    student_model.train()\n    \n    optimizer = torch.optim.AdamW(student_model.parameters(), lr=1e-3)\n    distillation_criterion = DistillationLoss(alpha=0.7, temperature=4.0)\n    \n    for epoch in range(num_epochs):\n        epoch_loss = 0.0\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            \n            # Get teacher predictions (no gradients needed)\n            with torch.no_grad():\n                teacher_logits = teacher_model(data)\n            \n            # Get student predictions\n            student_logits = student_model(data)\n            \n            # Calculate combined loss\n            total_loss, dist_loss, class_loss = distillation_criterion(\n                student_logits, teacher_logits, target\n            )\n            \n            # Backward pass\n            total_loss.backward()\n            optimizer.step()\n            \n            epoch_loss += total_loss.item()\n            \n            if batch_idx % 100 == 0:\n                print(f'Epoch {epoch}, Batch {batch_idx}')\n                print(f'  Total Loss: {total_loss.item():.4f}')\n                print(f'  Distillation Loss: {dist_loss.item():.4f}')\n                print(f'  Classification Loss: {class_loss.item():.4f}')\n        \n        print(f'Epoch {epoch} Average Loss: {epoch_loss/len(train_loader):.4f}')\n\n\n\nThe student learns from intermediate representations of the teacher:\nclass FeatureDistillationLoss(nn.Module):\n    \"\"\"\n    Distillation loss that includes intermediate feature matching\n    \"\"\"\n    def __init__(self, alpha=0.3, beta=0.3, temperature=4.0):\n        super().__init__()\n        self.alpha = alpha  # Weight for output distillation\n        self.beta = beta    # Weight for feature distillation\n        self.temperature = temperature\n        \n        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n        self.ce_loss = nn.CrossEntropyLoss()\n        self.mse_loss = nn.MSELoss()\n        \n    def forward(self, student_outputs, teacher_outputs, target):\n        student_logits, student_features = student_outputs\n        teacher_logits, teacher_features = teacher_outputs\n        \n        # Output distillation loss\n        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)\n        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)\n        output_dist_loss = self.kl_div(student_log_probs, teacher_probs)\n        \n        # Feature distillation loss\n        feature_dist_loss = 0\n        for s_feat, t_feat in zip(student_features, teacher_features):\n            # Align dimensions if necessary\n            if s_feat.shape != t_feat.shape:\n                s_feat = F.adaptive_avg_pool2d(s_feat, t_feat.shape[-2:])\n            feature_dist_loss += self.mse_loss(s_feat, t_feat.detach())\n        \n        # Classification loss\n        classification_loss = self.ce_loss(student_logits, target)\n        \n        # Combined loss\n        total_loss = (\n            self.alpha * output_dist_loss * (self.temperature ** 2) +\n            self.beta * feature_dist_loss +\n            (1 - self.alpha - self.beta) * classification_loss\n        )\n        \n        return total_loss\n\nclass TeacherStudentPair(nn.Module):\n    \"\"\"\n    Wrapper for teacher-student training with feature extraction\n    \"\"\"\n    def __init__(self, teacher_model, student_model):\n        super().__init__()\n        self.teacher = teacher_model\n        self.student = student_model\n        \n        # Hooks to capture intermediate features\n        self.teacher_features = []\n        self.student_features = []\n        \n        self._register_hooks()\n        \n    def _register_hooks(self):\n        \"\"\"Register forward hooks to capture intermediate features\"\"\"\n        def teacher_hook(module, input, output):\n            self.teacher_features.append(output)\n            \n        def student_hook(module, input, output):\n            self.student_features.append(output)\n        \n        # Register hooks on specific layers (e.g., after each transformer block)\n        for name, module in self.teacher.named_modules():\n            if 'layer' in name and 'attention' in name:\n                module.register_forward_hook(teacher_hook)\n                \n        for name, module in self.student.named_modules():\n            if 'layer' in name and 'attention' in name:\n                module.register_forward_hook(student_hook)\n    \n    def forward(self, x):\n        # Clear previous features\n        self.teacher_features.clear()\n        self.student_features.clear()\n        \n        # Forward pass through both models\n        with torch.no_grad():\n            teacher_output = self.teacher(x)\n        \n        student_output = self.student(x)\n        \n        return (student_output, self.student_features), (teacher_output, self.teacher_features)\n\n\n\nFor very large models, we can use a series of intermediate teachers:\ndef progressive_distillation(large_teacher, medium_student, small_student, train_loader):\n    \"\"\"\n    Progressive distillation: Large -&gt; Medium -&gt; Small\n    \"\"\"\n    print(\"Stage 1: Training medium student from large teacher...\")\n    \n    # First stage: Large teacher -&gt; Medium student\n    trained_medium = train_student_model(\n        teacher_model=large_teacher,\n        student_model=medium_student,\n        train_loader=train_loader,\n        num_epochs=15\n    )\n    \n    print(\"Stage 2: Training small student from medium teacher...\")\n    \n    # Second stage: Medium teacher -&gt; Small student\n    trained_small = train_student_model(\n        teacher_model=trained_medium,\n        student_model=small_student,\n        train_loader=train_loader,\n        num_epochs=15\n    )\n    \n    return trained_medium, trained_small\n\n\n\n\n\nLet me share how we applied these techniques at Mediacorp for our video recommendation system:\nclass ProductionOptimizedRecommender:\n    \"\"\"\n    Real-world implementation combining quantization and distillation\n    \"\"\"\n    def __init__(self, large_model_path):\n        # Load the large teacher model\n        self.teacher_model = torch.load(large_model_path)\n        self.teacher_model.eval()\n        \n        # Define smaller student architecture\n        self.student_model = self._create_student_architecture()\n        \n        # Optimization pipeline\n        self.optimization_pipeline = [\n            self._knowledge_distillation,\n            self._quantization,\n            self._model_pruning,\n            self._optimization_verification\n        ]\n    \n    def _create_student_architecture(self):\n        \"\"\"Create a more efficient student model\"\"\"\n        return nn.Sequential(\n            nn.Embedding(100000, 128),  # Smaller embedding dim\n            nn.TransformerEncoder(\n                nn.TransformerEncoderLayer(\n                    d_model=128,  # Reduced from 512\n                    nhead=4,      # Reduced from 8\n                    dim_feedforward=256,  # Reduced from 2048\n                    dropout=0.1\n                ),\n                num_layers=3  # Reduced from 6\n            ),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n    \n    def _knowledge_distillation(self):\n        \"\"\"Apply knowledge distillation\"\"\"\n        print(\"Applying knowledge distillation...\")\n        \n        # Custom loss for ranking tasks\n        class RankingDistillationLoss(nn.Module):\n            def __init__(self, temperature=3.0):\n                super().__init__()\n                self.temperature = temperature\n                self.mse_loss = nn.MSELoss()\n                \n            def forward(self, student_scores, teacher_scores, targets):\n                # Distillation loss for ranking scores\n                soft_teacher = F.softmax(teacher_scores / self.temperature, dim=1)\n                soft_student = F.log_softmax(student_scores / self.temperature, dim=1)\n                distillation_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean')\n                \n                # Direct score regression\n                score_loss = self.mse_loss(student_scores, teacher_scores.detach())\n                \n                # Ranking loss\n                ranking_loss = F.binary_cross_entropy_with_logits(student_scores, targets)\n                \n                return 0.4 * distillation_loss + 0.3 * score_loss + 0.3 * ranking_loss\n        \n        # Training implementation would go here\n        pass\n    \n    def _quantization(self):\n        \"\"\"Apply quantization to the distilled model\"\"\"\n        print(\"Applying quantization...\")\n        \n        self.student_model = torch.quantization.quantize_dynamic(\n            self.student_model,\n            {nn.Linear, nn.Embedding},\n            dtype=torch.qint8\n        )\n    \n    def _model_pruning(self):\n        \"\"\"Apply structured pruning for additional compression\"\"\"\n        print(\"Applying model pruning...\")\n        \n        # This is a simplified example - real pruning is more complex\n        for name, module in self.student_model.named_modules():\n            if isinstance(module, nn.Linear):\n                # Remove channels with smallest L1 norm\n                weights = module.weight.data\n                l1_norm = torch.sum(torch.abs(weights), dim=1)\n                _, indices = torch.topk(l1_norm, int(0.8 * len(l1_norm)))\n                \n                # Keep only top 80% of channels\n                module.weight.data = weights[indices]\n                if module.bias is not None:\n                    module.bias.data = module.bias.data[indices]\n    \n    def _optimization_verification(self):\n        \"\"\"Verify optimization results\"\"\"\n        print(\"Verifying optimization results...\")\n        \n        # Performance benchmarking\n        dummy_input = torch.randint(0, 1000, (32, 100))\n        \n        # Measure original model\n        start_time = time.time()\n        with torch.no_grad():\n            teacher_output = self.teacher_model(dummy_input)\n        teacher_time = time.time() - start_time\n        \n        # Measure optimized model\n        start_time = time.time()\n        with torch.no_grad():\n            student_output = self.student_model(dummy_input)\n        student_time = time.time() - start_time\n        \n        # Calculate metrics\n        speedup = teacher_time / student_time\n        size_reduction = self._calculate_model_size_reduction()\n        \n        print(f\"Speedup: {speedup:.2f}x\")\n        print(f\"Size reduction: {size_reduction:.2f}x\")\n        \n        return {\n            'speedup': speedup,\n            'size_reduction': size_reduction,\n            'teacher_latency': teacher_time,\n            'student_latency': student_time\n        }\n    \n    def optimize(self):\n        \"\"\"Run the complete optimization pipeline\"\"\"\n        results = {}\n        for step in self.optimization_pipeline:\n            step_result = step()\n            if step_result:\n                results.update(step_result)\n        \n        return results\n\n\n\n\n\nUse Quantization when: - You have limited memory constraints - Inference latency is critical - You can afford some accuracy loss (usually 1-3%) - Your model has many linear/conv layers\nUse Knowledge Distillation when: - You need to maintain high accuracy - You have access to unlabeled data - Model interpretability matters - You’re working with ensemble models\nCombine Both when: - Maximum compression is needed - You have computational budget for training - Production constraints are severe\n\n\n\nclass OptimizationBestPractices:\n    \"\"\"\n    Common mistakes and how to avoid them\n    \"\"\"\n    \n    @staticmethod\n    def calibration_data_selection():\n        \"\"\"\n        MISTAKE: Using random data for quantization calibration\n        SOLUTION: Use representative production data\n        \"\"\"\n        # Bad\n        calibration_data = torch.randn(100, 512)\n        \n        # Good - use actual data distribution\n        calibration_data = load_production_sample_data()\n        \n        return calibration_data\n    \n    @staticmethod\n    def temperature_tuning():\n        \"\"\"\n        MISTAKE: Using fixed temperature for all tasks\n        SOLUTION: Tune temperature based on task and data\n        \"\"\"\n        def find_optimal_temperature(teacher_outputs, validation_targets):\n            best_temp = 1.0\n            best_nll = float('inf')\n            \n            for temp in [1, 2, 3, 4, 5, 8, 10]:\n                soft_outputs = F.log_softmax(teacher_outputs / temp, dim=1)\n                nll = F.nll_loss(soft_outputs, validation_targets)\n                \n                if nll &lt; best_nll:\n                    best_nll = nll\n                    best_temp = temp\n            \n            return best_temp\n    \n    @staticmethod\n    def gradual_optimization():\n        \"\"\"\n        MISTAKE: Applying all optimizations at once\n        SOLUTION: Gradual optimization with validation at each step\n        \"\"\"\n        def optimize_gradually(model, validation_loader):\n            original_accuracy = evaluate_model(model, validation_loader)\n            print(f\"Original accuracy: {original_accuracy:.4f}\")\n            \n            # Step 1: Knowledge distillation\n            model = apply_distillation(model)\n            distilled_accuracy = evaluate_model(model, validation_loader)\n            print(f\"After distillation: {distilled_accuracy:.4f}\")\n            \n            if distilled_accuracy &lt; original_accuracy * 0.95:\n                print(\"Distillation caused too much accuracy loss, reverting...\")\n                return model\n            \n            # Step 2: Quantization\n            model = apply_quantization(model)\n            quantized_accuracy = evaluate_model(model, validation_loader)\n            print(f\"After quantization: {quantized_accuracy:.4f}\")\n            \n            if quantized_accuracy &lt; original_accuracy * 0.90:\n                print(\"Quantization caused too much accuracy loss, using different approach...\")\n                # Try different quantization strategy\n                pass\n            \n            return model\n\n\n\n\n\n\n\n\nclass NAS_Distillation:\n    \"\"\"\n    Using NAS to find optimal student architectures\n    \"\"\"\n    def search_student_architecture(self, teacher_model, search_space):\n        # Implementation would involve evolutionary algorithms\n        # or differentiable NAS to find optimal student\n        pass\n\n\n\ndef attention_transfer_loss(student_attention, teacher_attention):\n    \"\"\"\n    Transfer attention maps from teacher to student\n    \"\"\"\n    # Normalize attention maps\n    student_att = F.normalize(student_attention.view(-1), p=2, dim=0)\n    teacher_att = F.normalize(teacher_attention.view(-1), p=2, dim=0)\n    \n    # Attention transfer loss\n    return F.mse_loss(student_att, teacher_att)\n\n\n\n\nclass ProductionDeployment:\n    \"\"\"\n    Real considerations for production deployment\n    \"\"\"\n    \n    def __init__(self, optimized_model):\n        self.model = optimized_model\n        \n    def export_for_inference(self):\n        \"\"\"\n        Export model in production-ready format\n        \"\"\"\n        # TorchScript for PyTorch models\n        traced_model = torch.jit.trace(self.model, example_input)\n        traced_model.save(\"model.pt\")\n        \n        # ONNX for cross-platform deployment\n        torch.onnx.export(\n            self.model, \n            example_input, \n            \"model.onnx\",\n            export_params=True,\n            opset_version=11,\n            do_constant_folding=True\n        )\n        \n        return traced_model\n    \n    def benchmark_production_metrics(self):\n        \"\"\"\n        Comprehensive production benchmarking\n        \"\"\"\n        metrics = {\n            'latency_p50': self._measure_latency_percentile(50),\n            'latency_p95': self._measure_latency_percentile(95),\n            'latency_p99': self._measure_latency_percentile(99),\n            'throughput': self._measure_throughput(),\n            'memory_usage': self._measure_memory_usage(),\n            'cpu_usage': self._measure_cpu_usage(),\n            'accuracy': self._measure_accuracy()\n        }\n        \n        return metrics\n\n\n\n\nModel optimization is both an art and a science. The science lies in understanding the mathematical foundations of quantization and distillation. The art lies in knowing when and how to apply these techniques to achieve the right balance between performance, accuracy, and resource constraints.\nIn my experience building production ML systems, I’ve learned that:\n\nStart with your constraints: Understand your latency, memory, and accuracy requirements before choosing optimization techniques.\nMeasure everything: Always benchmark before and after optimization. What gets measured gets managed.\nGradual optimization works best: Apply techniques incrementally and validate at each step.\nProduction data matters: Use representative data for calibration and validation.\nMonitor continuously: Model performance can degrade in production due to data drift, so continuous monitoring is crucial.\n\nThe techniques covered in this post - from basic quantization to advanced knowledge distillation - are powerful tools in the ML engineer’s toolkit. But remember, they’re means to an end: building ML systems that deliver real value to users while operating efficiently at scale.\nAs we move toward even larger models and more constrained deployment environments, these optimization techniques will only become more important. The future belongs to engineers who can bridge the gap between state-of-the-art research and production-ready systems.\n\nHave you implemented quantization or knowledge distillation in your projects? What challenges did you face, and what worked well? I’d love to hear about your experiences in the comments below.\n\n\n\n\nHinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network.\nJacob, B., et al. (2018). Quantization and training of neural networks for efficient integer-arithmetic-only inference.\nRomero, A., et al. (2014). FitNets: Hints for thin deep nets.\nWu, J., Leng, C., Wang, Y., Hu, Q., & Cheng, J. (2016). Quantized convolutional neural networks for mobile devices.\nPyTorch Quantization Documentation\nTensorFlow Model Optimization Toolkit Documentation"
  },
  {
    "objectID": "posts/2025-01-09-quantization-distillation/index.html#the-production-reality",
    "href": "posts/2025-01-09-quantization-distillation/index.html#the-production-reality",
    "title": "Model Optimization Techniques: A Deep Dive into Quantization and Knowledge Distillation",
    "section": "",
    "text": "Let me start with a story from my experience at Mediacorp. We had built an excellent recommendation model using a large transformer architecture that achieved 40% CTR improvement in our offline tests. However, when we tried to deploy it to serve millions of users across web, mobile, and TV platforms, we faced several challenges:\n\nLatency: The model took 300ms per inference, far too slow for real-time recommendations\nMemory: Each model instance required 8GB of GPU memory, limiting our serving capacity\nCost: Running the model at scale would have cost us 10x our infrastructure budget\n\nThis is where model optimization techniques became not just nice-to-have, but absolutely critical for production deployment."
  },
  {
    "objectID": "posts/2025-01-09-quantization-distillation/index.html#part-i-quantization---precision-vs-performance",
    "href": "posts/2025-01-09-quantization-distillation/index.html#part-i-quantization---precision-vs-performance",
    "title": "Model Optimization Techniques: A Deep Dive into Quantization and Knowledge Distillation",
    "section": "",
    "text": "Quantization is the process of reducing the numerical precision of model weights and activations. Instead of using 32-bit floating point numbers (FP32), we can use 16-bit (FP16), 8-bit (INT8), or even 4-bit representations while maintaining acceptable model performance.\nThink of quantization like compressing a high-resolution image - you lose some detail, but the essential information remains, and the file size becomes much more manageable.\n\n\n\n\n\nThis is the simplest approach where we quantize a pre-trained model without any additional training.\nimport torch\nimport torch.quantization as quant\n\n# Example: Post-training quantization with PyTorch\ndef quantize_model_ptq(model, calibration_dataloader):\n    \"\"\"\n    Apply post-training quantization to a PyTorch model\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n    \n    # Specify quantization configuration\n    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n    \n    # Prepare model for quantization\n    model_prepared = torch.quantization.prepare(model)\n    \n    # Calibrate with representative data\n    with torch.no_grad():\n        for batch in calibration_dataloader:\n            model_prepared(batch)\n    \n    # Convert to quantized model\n    quantized_model = torch.quantization.convert(model_prepared)\n    \n    return quantized_model\n\n# Real-world usage example\ndef benchmark_quantization():\n    \"\"\"\n    Compare original vs quantized model performance\n    \"\"\"\n    original_model = YourTransformerModel()\n    quantized_model = quantize_model_ptq(original_model, calibration_loader)\n    \n    # Memory comparison\n    original_size = sum(p.numel() * p.element_size() for p in original_model.parameters())\n    quantized_size = sum(p.numel() * p.element_size() for p in quantized_model.parameters())\n    \n    print(f\"Original model size: {original_size / 1e6:.2f} MB\")\n    print(f\"Quantized model size: {quantized_size / 1e6:.2f} MB\")\n    print(f\"Compression ratio: {original_size / quantized_size:.2f}x\")\n    \n    # Latency comparison\n    import time\n    \n    # Warm up\n    dummy_input = torch.randn(1, 512, 768)\n    for _ in range(10):\n        _ = original_model(dummy_input)\n        _ = quantized_model(dummy_input)\n    \n    # Benchmark\n    start_time = time.time()\n    for _ in range(100):\n        _ = original_model(dummy_input)\n    original_time = time.time() - start_time\n    \n    start_time = time.time()\n    for _ in range(100):\n        _ = quantized_model(dummy_input)\n    quantized_time = time.time() - start_time\n    \n    print(f\"Original inference time: {original_time/100*1000:.2f} ms\")\n    print(f\"Quantized inference time: {quantized_time/100*1000:.2f} ms\")\n    print(f\"Speedup: {original_time/quantized_time:.2f}x\")\n\n\n\nQAT simulates quantization effects during training, allowing the model to adapt to reduced precision.\nimport torch.nn as nn\nimport torch.quantization as quant\n\nclass QuantizedTransformerBlock(nn.Module):\n    \"\"\"\n    A transformer block prepared for quantization-aware training\n    \"\"\"\n    def __init__(self, d_model, nhead, dim_feedforward):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(d_model, nhead)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n        self.feedforward = nn.Sequential(\n            nn.Linear(d_model, dim_feedforward),\n            nn.ReLU(),\n            nn.Linear(dim_feedforward, d_model)\n        )\n        \n        # Add quantization stubs\n        self.quant = quant.QuantStub()\n        self.dequant = quant.DeQuantStub()\n        \n    def forward(self, x):\n        x = self.quant(x)\n        \n        # Attention block with residual connection\n        attn_out, _ = self.attention(x, x, x)\n        x = self.norm1(x + attn_out)\n        \n        # Feedforward block with residual connection\n        ff_out = self.feedforward(x)\n        x = self.norm2(x + ff_out)\n        \n        x = self.dequant(x)\n        return x\n\ndef train_with_qat(model, train_loader, num_epochs=10):\n    \"\"\"\n    Training loop for quantization-aware training\n    \"\"\"\n    # Configure quantization settings\n    model.qconfig = quant.get_default_qat_qconfig('fbgemm')\n    \n    # Prepare model for QAT\n    model_prepared = quant.prepare_qat(model, inplace=False)\n    \n    # Training loop\n    optimizer = torch.optim.AdamW(model_prepared.parameters(), lr=1e-4)\n    criterion = nn.CrossEntropyLoss()\n    \n    model_prepared.train()\n    for epoch in range(num_epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model_prepared(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            \n            if batch_idx % 100 == 0:\n                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n    \n    # Convert to quantized model for deployment\n    model_prepared.eval()\n    quantized_model = quant.convert(model_prepared, inplace=False)\n    \n    return quantized_model\n\n\n\n\n\n\nPerfect for models with varying input sizes, like NLP applications:\ndef apply_dynamic_quantization(model):\n    \"\"\"\n    Apply dynamic quantization - weights are quantized, \n    activations are quantized dynamically during inference\n    \"\"\"\n    quantized_model = torch.quantization.quantize_dynamic(\n        model, \n        {nn.Linear, nn.LSTM, nn.GRU}, \n        dtype=torch.qint8\n    )\n    return quantized_model\n\n# Example usage with a BERT-style model\nclass OptimizedBERT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # Your BERT implementation here\n        pass\n    \n    def optimize_for_production(self):\n        \"\"\"\n        Apply multiple optimization techniques\n        \"\"\"\n        # First, apply dynamic quantization\n        self = torch.quantization.quantize_dynamic(\n            self, \n            {nn.Linear}, \n            dtype=torch.qint8\n        )\n        \n        # Then, fuse operations for better performance\n        self = torch.jit.script(self)\n        \n        return self\n\n\n\nUsing different precisions for different parts of the model:\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef train_with_mixed_precision(model, train_loader, num_epochs=10):\n    \"\"\"\n    Training with Automatic Mixed Precision for better performance\n    \"\"\"\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n    scaler = GradScaler()\n    criterion = nn.CrossEntropyLoss()\n    \n    model.train()\n    for epoch in range(num_epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            \n            # Forward pass with autocast\n            with autocast():\n                output = model(data)\n                loss = criterion(output, target)\n            \n            # Backward pass with gradient scaling\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            if batch_idx % 100 == 0:\n                print(f'Epoch {epoch}, Loss: {loss.item():.4f}')"
  },
  {
    "objectID": "posts/2025-01-09-quantization-distillation/index.html#part-ii-knowledge-distillation---learning-from-teachers",
    "href": "posts/2025-01-09-quantization-distillation/index.html#part-ii-knowledge-distillation---learning-from-teachers",
    "title": "Model Optimization Techniques: A Deep Dive into Quantization and Knowledge Distillation",
    "section": "",
    "text": "Knowledge distillation is like having an experienced mentor teach a junior developer. The mentor (teacher model) has years of experience and deep understanding, while the junior (student model) is eager to learn but needs guidance to develop similar intuition quickly.\nIn ML terms, we train a smaller, faster “student” model to mimic the behavior of a larger, more accurate “teacher” model.\n\n\n\n\n\nThe student learns from the teacher’s final outputs:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DistillationLoss(nn.Module):\n    \"\"\"\n    Combined loss for knowledge distillation\n    \"\"\"\n    def __init__(self, alpha=0.3, temperature=4.0):\n        super().__init__()\n        self.alpha = alpha  # Weight for distillation loss\n        self.temperature = temperature  # Temperature for softmax\n        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n        self.ce_loss = nn.CrossEntropyLoss()\n        \n    def forward(self, student_logits, teacher_logits, target):\n        # Distillation loss - student learns from teacher's soft predictions\n        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)\n        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)\n        distillation_loss = self.kl_div(student_log_probs, teacher_probs)\n        \n        # Standard classification loss - student learns from true labels\n        classification_loss = self.ce_loss(student_logits, target)\n        \n        # Combined loss\n        total_loss = (\n            self.alpha * distillation_loss * (self.temperature ** 2) +\n            (1 - self.alpha) * classification_loss\n        )\n        \n        return total_loss, distillation_loss, classification_loss\n\ndef train_student_model(teacher_model, student_model, train_loader, num_epochs=20):\n    \"\"\"\n    Train student model using knowledge distillation\n    \"\"\"\n    teacher_model.eval()  # Teacher is frozen\n    student_model.train()\n    \n    optimizer = torch.optim.AdamW(student_model.parameters(), lr=1e-3)\n    distillation_criterion = DistillationLoss(alpha=0.7, temperature=4.0)\n    \n    for epoch in range(num_epochs):\n        epoch_loss = 0.0\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            \n            # Get teacher predictions (no gradients needed)\n            with torch.no_grad():\n                teacher_logits = teacher_model(data)\n            \n            # Get student predictions\n            student_logits = student_model(data)\n            \n            # Calculate combined loss\n            total_loss, dist_loss, class_loss = distillation_criterion(\n                student_logits, teacher_logits, target\n            )\n            \n            # Backward pass\n            total_loss.backward()\n            optimizer.step()\n            \n            epoch_loss += total_loss.item()\n            \n            if batch_idx % 100 == 0:\n                print(f'Epoch {epoch}, Batch {batch_idx}')\n                print(f'  Total Loss: {total_loss.item():.4f}')\n                print(f'  Distillation Loss: {dist_loss.item():.4f}')\n                print(f'  Classification Loss: {class_loss.item():.4f}')\n        \n        print(f'Epoch {epoch} Average Loss: {epoch_loss/len(train_loader):.4f}')\n\n\n\nThe student learns from intermediate representations of the teacher:\nclass FeatureDistillationLoss(nn.Module):\n    \"\"\"\n    Distillation loss that includes intermediate feature matching\n    \"\"\"\n    def __init__(self, alpha=0.3, beta=0.3, temperature=4.0):\n        super().__init__()\n        self.alpha = alpha  # Weight for output distillation\n        self.beta = beta    # Weight for feature distillation\n        self.temperature = temperature\n        \n        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n        self.ce_loss = nn.CrossEntropyLoss()\n        self.mse_loss = nn.MSELoss()\n        \n    def forward(self, student_outputs, teacher_outputs, target):\n        student_logits, student_features = student_outputs\n        teacher_logits, teacher_features = teacher_outputs\n        \n        # Output distillation loss\n        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)\n        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)\n        output_dist_loss = self.kl_div(student_log_probs, teacher_probs)\n        \n        # Feature distillation loss\n        feature_dist_loss = 0\n        for s_feat, t_feat in zip(student_features, teacher_features):\n            # Align dimensions if necessary\n            if s_feat.shape != t_feat.shape:\n                s_feat = F.adaptive_avg_pool2d(s_feat, t_feat.shape[-2:])\n            feature_dist_loss += self.mse_loss(s_feat, t_feat.detach())\n        \n        # Classification loss\n        classification_loss = self.ce_loss(student_logits, target)\n        \n        # Combined loss\n        total_loss = (\n            self.alpha * output_dist_loss * (self.temperature ** 2) +\n            self.beta * feature_dist_loss +\n            (1 - self.alpha - self.beta) * classification_loss\n        )\n        \n        return total_loss\n\nclass TeacherStudentPair(nn.Module):\n    \"\"\"\n    Wrapper for teacher-student training with feature extraction\n    \"\"\"\n    def __init__(self, teacher_model, student_model):\n        super().__init__()\n        self.teacher = teacher_model\n        self.student = student_model\n        \n        # Hooks to capture intermediate features\n        self.teacher_features = []\n        self.student_features = []\n        \n        self._register_hooks()\n        \n    def _register_hooks(self):\n        \"\"\"Register forward hooks to capture intermediate features\"\"\"\n        def teacher_hook(module, input, output):\n            self.teacher_features.append(output)\n            \n        def student_hook(module, input, output):\n            self.student_features.append(output)\n        \n        # Register hooks on specific layers (e.g., after each transformer block)\n        for name, module in self.teacher.named_modules():\n            if 'layer' in name and 'attention' in name:\n                module.register_forward_hook(teacher_hook)\n                \n        for name, module in self.student.named_modules():\n            if 'layer' in name and 'attention' in name:\n                module.register_forward_hook(student_hook)\n    \n    def forward(self, x):\n        # Clear previous features\n        self.teacher_features.clear()\n        self.student_features.clear()\n        \n        # Forward pass through both models\n        with torch.no_grad():\n            teacher_output = self.teacher(x)\n        \n        student_output = self.student(x)\n        \n        return (student_output, self.student_features), (teacher_output, self.teacher_features)\n\n\n\nFor very large models, we can use a series of intermediate teachers:\ndef progressive_distillation(large_teacher, medium_student, small_student, train_loader):\n    \"\"\"\n    Progressive distillation: Large -&gt; Medium -&gt; Small\n    \"\"\"\n    print(\"Stage 1: Training medium student from large teacher...\")\n    \n    # First stage: Large teacher -&gt; Medium student\n    trained_medium = train_student_model(\n        teacher_model=large_teacher,\n        student_model=medium_student,\n        train_loader=train_loader,\n        num_epochs=15\n    )\n    \n    print(\"Stage 2: Training small student from medium teacher...\")\n    \n    # Second stage: Medium teacher -&gt; Small student\n    trained_small = train_student_model(\n        teacher_model=trained_medium,\n        student_model=small_student,\n        train_loader=train_loader,\n        num_epochs=15\n    )\n    \n    return trained_medium, trained_small"
  },
  {
    "objectID": "posts/2025-01-09-quantization-distillation/index.html#real-world-implementation-optimizing-a-recommendation-system",
    "href": "posts/2025-01-09-quantization-distillation/index.html#real-world-implementation-optimizing-a-recommendation-system",
    "title": "Model Optimization Techniques: A Deep Dive into Quantization and Knowledge Distillation",
    "section": "",
    "text": "Let me share how we applied these techniques at Mediacorp for our video recommendation system:\nclass ProductionOptimizedRecommender:\n    \"\"\"\n    Real-world implementation combining quantization and distillation\n    \"\"\"\n    def __init__(self, large_model_path):\n        # Load the large teacher model\n        self.teacher_model = torch.load(large_model_path)\n        self.teacher_model.eval()\n        \n        # Define smaller student architecture\n        self.student_model = self._create_student_architecture()\n        \n        # Optimization pipeline\n        self.optimization_pipeline = [\n            self._knowledge_distillation,\n            self._quantization,\n            self._model_pruning,\n            self._optimization_verification\n        ]\n    \n    def _create_student_architecture(self):\n        \"\"\"Create a more efficient student model\"\"\"\n        return nn.Sequential(\n            nn.Embedding(100000, 128),  # Smaller embedding dim\n            nn.TransformerEncoder(\n                nn.TransformerEncoderLayer(\n                    d_model=128,  # Reduced from 512\n                    nhead=4,      # Reduced from 8\n                    dim_feedforward=256,  # Reduced from 2048\n                    dropout=0.1\n                ),\n                num_layers=3  # Reduced from 6\n            ),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n    \n    def _knowledge_distillation(self):\n        \"\"\"Apply knowledge distillation\"\"\"\n        print(\"Applying knowledge distillation...\")\n        \n        # Custom loss for ranking tasks\n        class RankingDistillationLoss(nn.Module):\n            def __init__(self, temperature=3.0):\n                super().__init__()\n                self.temperature = temperature\n                self.mse_loss = nn.MSELoss()\n                \n            def forward(self, student_scores, teacher_scores, targets):\n                # Distillation loss for ranking scores\n                soft_teacher = F.softmax(teacher_scores / self.temperature, dim=1)\n                soft_student = F.log_softmax(student_scores / self.temperature, dim=1)\n                distillation_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean')\n                \n                # Direct score regression\n                score_loss = self.mse_loss(student_scores, teacher_scores.detach())\n                \n                # Ranking loss\n                ranking_loss = F.binary_cross_entropy_with_logits(student_scores, targets)\n                \n                return 0.4 * distillation_loss + 0.3 * score_loss + 0.3 * ranking_loss\n        \n        # Training implementation would go here\n        pass\n    \n    def _quantization(self):\n        \"\"\"Apply quantization to the distilled model\"\"\"\n        print(\"Applying quantization...\")\n        \n        self.student_model = torch.quantization.quantize_dynamic(\n            self.student_model,\n            {nn.Linear, nn.Embedding},\n            dtype=torch.qint8\n        )\n    \n    def _model_pruning(self):\n        \"\"\"Apply structured pruning for additional compression\"\"\"\n        print(\"Applying model pruning...\")\n        \n        # This is a simplified example - real pruning is more complex\n        for name, module in self.student_model.named_modules():\n            if isinstance(module, nn.Linear):\n                # Remove channels with smallest L1 norm\n                weights = module.weight.data\n                l1_norm = torch.sum(torch.abs(weights), dim=1)\n                _, indices = torch.topk(l1_norm, int(0.8 * len(l1_norm)))\n                \n                # Keep only top 80% of channels\n                module.weight.data = weights[indices]\n                if module.bias is not None:\n                    module.bias.data = module.bias.data[indices]\n    \n    def _optimization_verification(self):\n        \"\"\"Verify optimization results\"\"\"\n        print(\"Verifying optimization results...\")\n        \n        # Performance benchmarking\n        dummy_input = torch.randint(0, 1000, (32, 100))\n        \n        # Measure original model\n        start_time = time.time()\n        with torch.no_grad():\n            teacher_output = self.teacher_model(dummy_input)\n        teacher_time = time.time() - start_time\n        \n        # Measure optimized model\n        start_time = time.time()\n        with torch.no_grad():\n            student_output = self.student_model(dummy_input)\n        student_time = time.time() - start_time\n        \n        # Calculate metrics\n        speedup = teacher_time / student_time\n        size_reduction = self._calculate_model_size_reduction()\n        \n        print(f\"Speedup: {speedup:.2f}x\")\n        print(f\"Size reduction: {size_reduction:.2f}x\")\n        \n        return {\n            'speedup': speedup,\n            'size_reduction': size_reduction,\n            'teacher_latency': teacher_time,\n            'student_latency': student_time\n        }\n    \n    def optimize(self):\n        \"\"\"Run the complete optimization pipeline\"\"\"\n        results = {}\n        for step in self.optimization_pipeline:\n            step_result = step()\n            if step_result:\n                results.update(step_result)\n        \n        return results"
  },
  {
    "objectID": "posts/2025-01-09-quantization-distillation/index.html#best-practices-and-gotchas",
    "href": "posts/2025-01-09-quantization-distillation/index.html#best-practices-and-gotchas",
    "title": "Model Optimization Techniques: A Deep Dive into Quantization and Knowledge Distillation",
    "section": "",
    "text": "Use Quantization when: - You have limited memory constraints - Inference latency is critical - You can afford some accuracy loss (usually 1-3%) - Your model has many linear/conv layers\nUse Knowledge Distillation when: - You need to maintain high accuracy - You have access to unlabeled data - Model interpretability matters - You’re working with ensemble models\nCombine Both when: - Maximum compression is needed - You have computational budget for training - Production constraints are severe\n\n\n\nclass OptimizationBestPractices:\n    \"\"\"\n    Common mistakes and how to avoid them\n    \"\"\"\n    \n    @staticmethod\n    def calibration_data_selection():\n        \"\"\"\n        MISTAKE: Using random data for quantization calibration\n        SOLUTION: Use representative production data\n        \"\"\"\n        # Bad\n        calibration_data = torch.randn(100, 512)\n        \n        # Good - use actual data distribution\n        calibration_data = load_production_sample_data()\n        \n        return calibration_data\n    \n    @staticmethod\n    def temperature_tuning():\n        \"\"\"\n        MISTAKE: Using fixed temperature for all tasks\n        SOLUTION: Tune temperature based on task and data\n        \"\"\"\n        def find_optimal_temperature(teacher_outputs, validation_targets):\n            best_temp = 1.0\n            best_nll = float('inf')\n            \n            for temp in [1, 2, 3, 4, 5, 8, 10]:\n                soft_outputs = F.log_softmax(teacher_outputs / temp, dim=1)\n                nll = F.nll_loss(soft_outputs, validation_targets)\n                \n                if nll &lt; best_nll:\n                    best_nll = nll\n                    best_temp = temp\n            \n            return best_temp\n    \n    @staticmethod\n    def gradual_optimization():\n        \"\"\"\n        MISTAKE: Applying all optimizations at once\n        SOLUTION: Gradual optimization with validation at each step\n        \"\"\"\n        def optimize_gradually(model, validation_loader):\n            original_accuracy = evaluate_model(model, validation_loader)\n            print(f\"Original accuracy: {original_accuracy:.4f}\")\n            \n            # Step 1: Knowledge distillation\n            model = apply_distillation(model)\n            distilled_accuracy = evaluate_model(model, validation_loader)\n            print(f\"After distillation: {distilled_accuracy:.4f}\")\n            \n            if distilled_accuracy &lt; original_accuracy * 0.95:\n                print(\"Distillation caused too much accuracy loss, reverting...\")\n                return model\n            \n            # Step 2: Quantization\n            model = apply_quantization(model)\n            quantized_accuracy = evaluate_model(model, validation_loader)\n            print(f\"After quantization: {quantized_accuracy:.4f}\")\n            \n            if quantized_accuracy &lt; original_accuracy * 0.90:\n                print(\"Quantization caused too much accuracy loss, using different approach...\")\n                # Try different quantization strategy\n                pass\n            \n            return model"
  },
  {
    "objectID": "posts/2025-01-09-quantization-distillation/index.html#advanced-topics-and-future-directions",
    "href": "posts/2025-01-09-quantization-distillation/index.html#advanced-topics-and-future-directions",
    "title": "Model Optimization Techniques: A Deep Dive into Quantization and Knowledge Distillation",
    "section": "",
    "text": "class NAS_Distillation:\n    \"\"\"\n    Using NAS to find optimal student architectures\n    \"\"\"\n    def search_student_architecture(self, teacher_model, search_space):\n        # Implementation would involve evolutionary algorithms\n        # or differentiable NAS to find optimal student\n        pass\n\n\n\ndef attention_transfer_loss(student_attention, teacher_attention):\n    \"\"\"\n    Transfer attention maps from teacher to student\n    \"\"\"\n    # Normalize attention maps\n    student_att = F.normalize(student_attention.view(-1), p=2, dim=0)\n    teacher_att = F.normalize(teacher_attention.view(-1), p=2, dim=0)\n    \n    # Attention transfer loss\n    return F.mse_loss(student_att, teacher_att)\n\n\n\n\nclass ProductionDeployment:\n    \"\"\"\n    Real considerations for production deployment\n    \"\"\"\n    \n    def __init__(self, optimized_model):\n        self.model = optimized_model\n        \n    def export_for_inference(self):\n        \"\"\"\n        Export model in production-ready format\n        \"\"\"\n        # TorchScript for PyTorch models\n        traced_model = torch.jit.trace(self.model, example_input)\n        traced_model.save(\"model.pt\")\n        \n        # ONNX for cross-platform deployment\n        torch.onnx.export(\n            self.model, \n            example_input, \n            \"model.onnx\",\n            export_params=True,\n            opset_version=11,\n            do_constant_folding=True\n        )\n        \n        return traced_model\n    \n    def benchmark_production_metrics(self):\n        \"\"\"\n        Comprehensive production benchmarking\n        \"\"\"\n        metrics = {\n            'latency_p50': self._measure_latency_percentile(50),\n            'latency_p95': self._measure_latency_percentile(95),\n            'latency_p99': self._measure_latency_percentile(99),\n            'throughput': self._measure_throughput(),\n            'memory_usage': self._measure_memory_usage(),\n            'cpu_usage': self._measure_cpu_usage(),\n            'accuracy': self._measure_accuracy()\n        }\n        \n        return metrics"
  },
  {
    "objectID": "posts/2025-01-09-quantization-distillation/index.html#conclusion-the-art-and-science-of-model-optimization",
    "href": "posts/2025-01-09-quantization-distillation/index.html#conclusion-the-art-and-science-of-model-optimization",
    "title": "Model Optimization Techniques: A Deep Dive into Quantization and Knowledge Distillation",
    "section": "",
    "text": "Model optimization is both an art and a science. The science lies in understanding the mathematical foundations of quantization and distillation. The art lies in knowing when and how to apply these techniques to achieve the right balance between performance, accuracy, and resource constraints.\nIn my experience building production ML systems, I’ve learned that:\n\nStart with your constraints: Understand your latency, memory, and accuracy requirements before choosing optimization techniques.\nMeasure everything: Always benchmark before and after optimization. What gets measured gets managed.\nGradual optimization works best: Apply techniques incrementally and validate at each step.\nProduction data matters: Use representative data for calibration and validation.\nMonitor continuously: Model performance can degrade in production due to data drift, so continuous monitoring is crucial.\n\nThe techniques covered in this post - from basic quantization to advanced knowledge distillation - are powerful tools in the ML engineer’s toolkit. But remember, they’re means to an end: building ML systems that deliver real value to users while operating efficiently at scale.\nAs we move toward even larger models and more constrained deployment environments, these optimization techniques will only become more important. The future belongs to engineers who can bridge the gap between state-of-the-art research and production-ready systems.\n\nHave you implemented quantization or knowledge distillation in your projects? What challenges did you face, and what worked well? I’d love to hear about your experiences in the comments below."
  },
  {
    "objectID": "posts/2025-01-09-quantization-distillation/index.html#references-and-further-reading",
    "href": "posts/2025-01-09-quantization-distillation/index.html#references-and-further-reading",
    "title": "Model Optimization Techniques: A Deep Dive into Quantization and Knowledge Distillation",
    "section": "",
    "text": "Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network.\nJacob, B., et al. (2018). Quantization and training of neural networks for efficient integer-arithmetic-only inference.\nRomero, A., et al. (2014). FitNets: Hints for thin deep nets.\nWu, J., Leng, C., Wang, Y., Hu, Q., & Cheng, J. (2016). Quantized convolutional neural networks for mobile devices.\nPyTorch Quantization Documentation\nTensorFlow Model Optimization Toolkit Documentation"
  },
  {
    "objectID": "posts/2025-01-01-azure-ml/index.html",
    "href": "posts/2025-01-01-azure-ml/index.html",
    "title": "Azure Machine Learning",
    "section": "",
    "text": "youtube: https://www.youtube.com/watch?v=uIq-aZsiKog"
  },
  {
    "objectID": "posts/2025-01-01-azure-ml/index.html#compute-resource",
    "href": "posts/2025-01-01-azure-ml/index.html#compute-resource",
    "title": "Azure Machine Learning",
    "section": "Compute Resource",
    "text": "Compute Resource\nA compute resource can be any service in Azure that provides you with computing power, such as VMs, managed clusters of VMs (Azure Batch, Azure Databricks, and so on), container execution engines (Azure Kubernetes Services, Azure Container Instance, Azure Functions, Azure IoT Edge, and so on), or hybrid compute services such as App Service. This service is usually used for experimentation or is managed from an ML infrastructure service."
  },
  {
    "objectID": "posts/2025-01-01-azure-ml/index.html#ml-infrastructure-service",
    "href": "posts/2025-01-01-azure-ml/index.html#ml-infrastructure-service",
    "title": "Azure Machine Learning",
    "section": "ML infrastructure service",
    "text": "ML infrastructure service\nAn ML infrastructure service helps you implement, orchestrate, automate, and optimize your ML training, pipelines, and deployments. Using such a service, you would usually implement your own preprocessing and ML algorithms using your own frameworks. However, the service would support you with infrastructure for the training, optimization and deployment process. Azure Machine Learning is a service in Azure that falls into this category and will be the service that we use throughout this book."
  },
  {
    "objectID": "posts/2025-01-01-azure-ml/index.html#ml-modeling-service",
    "href": "posts/2025-01-01-azure-ml/index.html#ml-modeling-service",
    "title": "Azure Machine Learning",
    "section": "ML modeling service",
    "text": "ML modeling service\nFinally, an ML modeling service is a service that helps you to create or use ML models without writing your own code. Services such as Cognitive Services, Azure Automated Machine Learning, Azure Machine Learning designer, and Custom Vision can be found in this category."
  },
  {
    "objectID": "posts/2025-01-01-azure-ml/index.html#choosing-a-ml-service",
    "href": "posts/2025-01-01-azure-ml/index.html#choosing-a-ml-service",
    "title": "Azure Machine Learning",
    "section": "Choosing a ML Service",
    "text": "Choosing a ML Service\n\n\n\nAzure ML Pipeline Selection\n\n\nIn Azure, you have various choices to build your end-to-end ML pipelines for training, optimizing, and deploying custom ML models:\n\nBuild your own tools\nUse open source tools, such as Azure Databricks with ML Flow\nUse a GUI tool, such as Azure Machine Learning designer\nUse Azure Machine Learning"
  },
  {
    "objectID": "posts/2020-01-28-recommender-systems/index.html",
    "href": "posts/2020-01-28-recommender-systems/index.html",
    "title": "Recommender Systems - Blogs from Kaggle",
    "section": "",
    "text": "https://github.com/grahamjenson/list_of_recommender_systems\nhttps://www.pinterest.de/dataliftoff/recommender-systems/python-libraries/\nhttps://www.kaggle.com/gunnvant/building-content-recommender-tutorial"
  },
  {
    "objectID": "posts/2020-01-28-recommender-systems/index.html#links",
    "href": "posts/2020-01-28-recommender-systems/index.html#links",
    "title": "Recommender Systems - Blogs from Kaggle",
    "section": "",
    "text": "https://github.com/grahamjenson/list_of_recommender_systems\nhttps://www.pinterest.de/dataliftoff/recommender-systems/python-libraries/\nhttps://www.kaggle.com/gunnvant/building-content-recommender-tutorial"
  },
  {
    "objectID": "posts/2020-01-28-recommender-systems/index.html#key-concepts",
    "href": "posts/2020-01-28-recommender-systems/index.html#key-concepts",
    "title": "Recommender Systems - Blogs from Kaggle",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nRecommendation engines used by the official ted page, will be a degrees of magnitude more sophisticated than what I can demonstrate here and would also involve use of some sort of historical user-item interaction data.\nGenerate recommendations just using content when you don’t have any user-item interaction data\nWhen you are starting out new and still want to provide the consumers of your content relevant contextual recommendations\nRecommend talks based on the similarity of their content, the first thing I will have to do is to, create a representation of the transcripts that are amenable to comparison. One way of doing this is to create a tfidf vector for each transcript.\n\n\nCore Functions:\n\nTfidfVectorizer\ncosine_similarity(matrix)\nget_similar_articles\n\n\n\nAdditional Improvements:\n\ntf-idf with unigrams, you can try using bigrams and see if you get better results.\nTry using pre-trained word vectors such as word2vec to create vector representation of just the Titles and try to find similarity using cosine distance"
  },
  {
    "objectID": "posts/2020-01-28-recommender-systems/index.html#amazon-reviews-recommender-system",
    "href": "posts/2020-01-28-recommender-systems/index.html#amazon-reviews-recommender-system",
    "title": "Recommender Systems - Blogs from Kaggle",
    "section": "Amazon Reviews Recommender System",
    "text": "Amazon Reviews Recommender System\nhttps://www.kaggle.com/saurav9786/recommender-system-using-amazon-reviews\n\nAmazon uses currently item-item collaborative filtering, which scales to massive datasets and produces high quality recommendation system in the real time.\n\n\nThis system is a kind of information filtering system which seeks to predict the “rating” or preferences which user is interested in.\n\n\nTypes of Recommendation Systems\nThere are mainly 6 types of the recommendations systems:\n\nPopularity based systems: Works by recommending items viewed and purchased by most people and are rated high. It is not a personalized recommendation.\nClassification model based: Works by understanding the features of the user and applying the classification algorithm to decide whether the user is interested or not in the product.\nContent based recommendations: Based on the information on the contents of the item rather than on the user opinions. The main idea is if the user likes an item then he or she will like the “other” similar item.\nCollaborative Filtering: Based on assumption that people like things similar to other things they like, and things that are liked by other people with similar taste. It is mainly of two types: a) User-User b) Item-Item\nHybrid Approaches: This system approach is to combine collaborative filtering, content-based filtering, and other approaches.\nAssociation rule mining: Association rules capture the relationships between items based on their patterns of co-occurrence across transactions.\n\n\n\nAttribute Information:\n\nuserId: Every user identified with a unique id\nproductId: Every product identified with a unique id\nRating: Rating of the corresponding product by the corresponding user\ntimestamp: Time of the rating (ignore this column for this exercise)\n\n\n\nImplementation Steps:\n\nPopularity based\nCheck the distribution of the rating\nOn the basis of Rating.count()\n\nCF is based on the idea that the best recommendations come from people who have similar tastes. In other words, it uses historical item ratings of like-minded people to predict how someone would rate an item. Collaborative filtering has two sub-categories that are generally called memory based and model-based approaches.\n\nMemory-based collaborative filtering system\nfrom surprise import KNNWithMeans\nParameter: user_based true/false to switch between user-based or item-based collaborative filtering\n\n\nModel-based collaborative filtering system\nThe advantage of these methods is that they are able to recommend a larger number of items to a larger number of users, compared to other methods like memory based approach. They have large coverage, even when working with large sparse matrices.\nnew_df1.pivot_table\nfrom sklearn.decomposition import TruncatedSVD\nnp.corrcoef(decomposed_matrix)"
  },
  {
    "objectID": "posts/2020-01-28-recommender-systems/index.html#tutorial-recommendation-systems",
    "href": "posts/2020-01-28-recommender-systems/index.html#tutorial-recommendation-systems",
    "title": "Recommender Systems - Blogs from Kaggle",
    "section": "Tutorial: Recommendation Systems",
    "text": "Tutorial: Recommendation Systems\nhttps://www.kaggle.com/kanncaa1/recommendation-systems-tutorial\n\nUser Based Collaborative Filtering\n\nCollaborative filtering is making recommend according to combination of your experience and experiences of other people.\nFirst we need to make user vs item matrix.\nEach row is users and each columns are items like movie, product or websites\nSecondly, computes similarity scores between users.\nEach row is users and each row is vector.\nCompute similarity of these rows (users).\nThirdly, find users who are similar to you based on past behaviours\nFinally, it suggests that you are not experienced before.\n\n\nExample:\n\nThink that there are two people\nFirst one watched 2 movies that are lord of the rings and hobbit\nSecond one watched only lord of the rings movie\nUser based collaborative filtering computes similarity of these two people and sees both are watched a lord of the rings.\nThen it recommends hobbit movie to second one\n\n\n\nProblems with User Based Collaborative Filtering:\n\nIn this system, each row of matrix is user. Therefore, comparing and finding similarity between of them is computationally hard and spend too much computational power.\nAlso, habits of people can be changed. Therefore making correct and useful recommendation can be hard in time.\n\n\n\n\nItem Based Collaborative Filtering\n\nIn this system, instead of finding relationship between users, used items like movies or stuffs are compared with each others.\nIn user based recommendation systems, habits of users can be changed. This situation makes hard to recommendation. However, in item based recommendation systems, movies or stuffs does not change. Therefore recommendation is easier.\nOn the other hand, there are almost 7 billion people all over the world. Comparing people increases the computational power. However, if items are compared, computational power is less.\n\n\nImplementation:\n[\"userId\",\"movieId\",\"rating\"]\ndata.pivot_table(index = [\"userId\"],columns = [\"title\"],values = \"rating\")\nmovie_watched = pivot_table[\"Bad Boys (1995)\"]   \nsimilarity_with_other_movies = pivot_table.corrwith(movie_watched)  # find correlation\nsimilarity_with_other_movies = similarity_with_other_movies.sort_values(ascending=False)"
  },
  {
    "objectID": "posts/2020-01-28-recommender-systems/index.html#goodreads-collaborative-recommender-system",
    "href": "posts/2020-01-28-recommender-systems/index.html#goodreads-collaborative-recommender-system",
    "title": "Recommender Systems - Blogs from Kaggle",
    "section": "Goodreads Collaborative Recommender System",
    "text": "Goodreads Collaborative Recommender System\nhttps://www.kaggle.com/sriharshavogeti/collaborative-recommender-system-on-goodreads\n\nNaive item-similarity based recommender system\nDataset structure: | book_id | user_id | rating | |————-|————-|————|\nCreate dictionary corresponding to each book id with its mapping and its key value as user_ids:rating\ndictVectorizer = DictVectorizer(sparse=True)\nvector = dictVectorizer.fit_transform(listOfDictonaries)\ncosine_similarity(vector)\nnp.argsort(pairwiseSimilarity[row])[-7:-2][::-1]"
  },
  {
    "objectID": "posts/2020-01-28-recommender-systems/index.html#deep-recommender-systems",
    "href": "posts/2020-01-28-recommender-systems/index.html#deep-recommender-systems",
    "title": "Recommender Systems - Blogs from Kaggle",
    "section": "Deep Recommender Systems",
    "text": "Deep Recommender Systems\nhttps://www.kaggle.com/morrisb/how-to-recommend-anything-deep-recommender\n\n11. Recommendation Engines\n\n11.1. Mean Rating\n11.2. Weighted Mean Rating\n11.3. Cosine User-User Similarity\n11.4. Cosine TFIDF Movie Description Similarity\n11.5. Matrix Factorisation With Keras And Gradient Descent\n11.6. Deep Learning With Keras\n11.7. Deep Hybrid System With Metadata And Keras\n\ndf_train.pivot_table(index='User', columns='Movie', values='Rating')"
  },
  {
    "objectID": "posts/2020-01-14-markdown-example/index.html",
    "href": "posts/2020-01-14-markdown-example/index.html",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "Quarto requires blog post files to be organized in folders or named with dates. This post demonstrates basic markdown formatting.\n\n\n\nYou can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule:\n\n\n\n\nHere’s a list:\n\nitem 1\nitem 2\n\nAnd a numbered list:\n\nitem 1\nitem 2\n\n\n\n\n\nThis is a quotation\n\n\n\n\n\n\n\nNote\n\n\n\nYou can include note callouts in Quarto\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can include tip callouts too\n\n\n\n\n\n\n\n\nQuarto Logo\n\n\n\n\n\nYou can format text and code per usual\nGeneral preformatted text:\n# Do a thing\ndo_thing()\nPython code and output:\n# Prints '2'\nprint(1+1)\nFormatting text as shell commands:\necho \"hello world\"\n./some_script.sh --option \"value\"\nwget https://example.com/cat_photo1.png\nFormatting text as YAML:\nkey: value\n- another_key: \"another value\"\n\n\n\n\n\n\nColumn 1\nColumn 2\n\n\n\n\nA thing\nAnother thing"
  },
  {
    "objectID": "posts/2020-01-14-markdown-example/index.html#basic-setup",
    "href": "posts/2020-01-14-markdown-example/index.html#basic-setup",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "Quarto requires blog post files to be organized in folders or named with dates. This post demonstrates basic markdown formatting."
  },
  {
    "objectID": "posts/2020-01-14-markdown-example/index.html#basic-formatting",
    "href": "posts/2020-01-14-markdown-example/index.html#basic-formatting",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule:"
  },
  {
    "objectID": "posts/2020-01-14-markdown-example/index.html#lists",
    "href": "posts/2020-01-14-markdown-example/index.html#lists",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "Here’s a list:\n\nitem 1\nitem 2\n\nAnd a numbered list:\n\nitem 1\nitem 2"
  },
  {
    "objectID": "posts/2020-01-14-markdown-example/index.html#boxes-and-stuff",
    "href": "posts/2020-01-14-markdown-example/index.html#boxes-and-stuff",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "This is a quotation\n\n\n\n\n\n\n\nNote\n\n\n\nYou can include note callouts in Quarto\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can include tip callouts too"
  },
  {
    "objectID": "posts/2020-01-14-markdown-example/index.html#images",
    "href": "posts/2020-01-14-markdown-example/index.html#images",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "Quarto Logo"
  },
  {
    "objectID": "posts/2020-01-14-markdown-example/index.html#code",
    "href": "posts/2020-01-14-markdown-example/index.html#code",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "You can format text and code per usual\nGeneral preformatted text:\n# Do a thing\ndo_thing()\nPython code and output:\n# Prints '2'\nprint(1+1)\nFormatting text as shell commands:\necho \"hello world\"\n./some_script.sh --option \"value\"\nwget https://example.com/cat_photo1.png\nFormatting text as YAML:\nkey: value\n- another_key: \"another value\""
  },
  {
    "objectID": "posts/2020-01-14-markdown-example/index.html#tables",
    "href": "posts/2020-01-14-markdown-example/index.html#tables",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "Column 1\nColumn 2\n\n\n\n\nA thing\nAnother thing"
  },
  {
    "objectID": "posts/2020-01-14-markdown-example/index.html#footnotes",
    "href": "posts/2020-01-14-markdown-example/index.html#footnotes",
    "title": "An Example Markdown Post",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is the footnote.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI/ML Notes",
    "section": "",
    "text": "Welcome to my AI/ML blog! This is a collection of my notes and insights on Machine Learning, Deep Learning, and related topics."
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "AI/ML Notes",
    "section": "Latest Posts",
    "text": "Latest Posts"
  }
]