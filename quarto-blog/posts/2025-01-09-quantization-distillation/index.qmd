---
title: "Model Optimization Techniques: A Deep Dive into Quantization and Knowledge Distillation"
author: "Rakshit Sakhuja"
date: "2025-01-09"
categories: [Deep Learning, Model Optimization, MLOps, Production ML]
image: "quantization_distillation.png"
description: "An exhaustive guide to model compression techniques that every ML engineer should know - from basic quantization to advanced distillation methods with real-world implementations."
---

# Model Optimization: Making Large Models Production-Ready

In the era of large language models and complex neural networks, the gap between model capability and deployment constraints has never been wider. While a 70B parameter model might achieve state-of-the-art results, deploying it in production often requires significant optimization. This comprehensive guide explores two of the most effective model compression techniques: **quantization** and **knowledge distillation**.

## The Production Reality

Let me start with a story from my experience at Mediacorp. We had built an excellent recommendation model using a large transformer architecture that achieved 40% CTR improvement in our offline tests. However, when we tried to deploy it to serve millions of users across web, mobile, and TV platforms, we faced several challenges:

- **Latency**: The model took 300ms per inference, far too slow for real-time recommendations
- **Memory**: Each model instance required 8GB of GPU memory, limiting our serving capacity
- **Cost**: Running the model at scale would have cost us 10x our infrastructure budget

This is where model optimization techniques became not just nice-to-have, but absolutely critical for production deployment.

## Part I: Quantization - Precision vs Performance

### What is Quantization?

Quantization is the process of reducing the numerical precision of model weights and activations. Instead of using 32-bit floating point numbers (FP32), we can use 16-bit (FP16), 8-bit (INT8), or even 4-bit representations while maintaining acceptable model performance.

Think of quantization like compressing a high-resolution image - you lose some detail, but the essential information remains, and the file size becomes much more manageable.

### Types of Quantization

#### 1. Post-Training Quantization (PTQ)

This is the simplest approach where we quantize a pre-trained model without any additional training.

```python
import torch
import torch.quantization as quant

# Example: Post-training quantization with PyTorch
def quantize_model_ptq(model, calibration_dataloader):
    """
    Apply post-training quantization to a PyTorch model
    """
    # Set model to evaluation mode
    model.eval()
    
    # Specify quantization configuration
    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
    
    # Prepare model for quantization
    model_prepared = torch.quantization.prepare(model)
    
    # Calibrate with representative data
    with torch.no_grad():
        for batch in calibration_dataloader:
            model_prepared(batch)
    
    # Convert to quantized model
    quantized_model = torch.quantization.convert(model_prepared)
    
    return quantized_model

# Real-world usage example
def benchmark_quantization():
    """
    Compare original vs quantized model performance
    """
    original_model = YourTransformerModel()
    quantized_model = quantize_model_ptq(original_model, calibration_loader)
    
    # Memory comparison
    original_size = sum(p.numel() * p.element_size() for p in original_model.parameters())
    quantized_size = sum(p.numel() * p.element_size() for p in quantized_model.parameters())
    
    print(f"Original model size: {original_size / 1e6:.2f} MB")
    print(f"Quantized model size: {quantized_size / 1e6:.2f} MB")
    print(f"Compression ratio: {original_size / quantized_size:.2f}x")
    
    # Latency comparison
    import time
    
    # Warm up
    dummy_input = torch.randn(1, 512, 768)
    for _ in range(10):
        _ = original_model(dummy_input)
        _ = quantized_model(dummy_input)
    
    # Benchmark
    start_time = time.time()
    for _ in range(100):
        _ = original_model(dummy_input)
    original_time = time.time() - start_time
    
    start_time = time.time()
    for _ in range(100):
        _ = quantized_model(dummy_input)
    quantized_time = time.time() - start_time
    
    print(f"Original inference time: {original_time/100*1000:.2f} ms")
    print(f"Quantized inference time: {quantized_time/100*1000:.2f} ms")
    print(f"Speedup: {original_time/quantized_time:.2f}x")
```

#### 2. Quantization-Aware Training (QAT)

QAT simulates quantization effects during training, allowing the model to adapt to reduced precision.

```python
import torch.nn as nn
import torch.quantization as quant

class QuantizedTransformerBlock(nn.Module):
    """
    A transformer block prepared for quantization-aware training
    """
    def __init__(self, d_model, nhead, dim_feedforward):
        super().__init__()
        self.attention = nn.MultiheadAttention(d_model, nhead)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        self.feedforward = nn.Sequential(
            nn.Linear(d_model, dim_feedforward),
            nn.ReLU(),
            nn.Linear(dim_feedforward, d_model)
        )
        
        # Add quantization stubs
        self.quant = quant.QuantStub()
        self.dequant = quant.DeQuantStub()
        
    def forward(self, x):
        x = self.quant(x)
        
        # Attention block with residual connection
        attn_out, _ = self.attention(x, x, x)
        x = self.norm1(x + attn_out)
        
        # Feedforward block with residual connection
        ff_out = self.feedforward(x)
        x = self.norm2(x + ff_out)
        
        x = self.dequant(x)
        return x

def train_with_qat(model, train_loader, num_epochs=10):
    """
    Training loop for quantization-aware training
    """
    # Configure quantization settings
    model.qconfig = quant.get_default_qat_qconfig('fbgemm')
    
    # Prepare model for QAT
    model_prepared = quant.prepare_qat(model, inplace=False)
    
    # Training loop
    optimizer = torch.optim.AdamW(model_prepared.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss()
    
    model_prepared.train()
    for epoch in range(num_epochs):
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            output = model_prepared(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            if batch_idx % 100 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')
    
    # Convert to quantized model for deployment
    model_prepared.eval()
    quantized_model = quant.convert(model_prepared, inplace=False)
    
    return quantized_model
```

### Advanced Quantization Techniques

#### Dynamic Quantization

Perfect for models with varying input sizes, like NLP applications:

```python
def apply_dynamic_quantization(model):
    """
    Apply dynamic quantization - weights are quantized, 
    activations are quantized dynamically during inference
    """
    quantized_model = torch.quantization.quantize_dynamic(
        model, 
        {nn.Linear, nn.LSTM, nn.GRU}, 
        dtype=torch.qint8
    )
    return quantized_model

# Example usage with a BERT-style model
class OptimizedBERT(nn.Module):
    def __init__(self, config):
        super().__init__()
        # Your BERT implementation here
        pass
    
    def optimize_for_production(self):
        """
        Apply multiple optimization techniques
        """
        # First, apply dynamic quantization
        self = torch.quantization.quantize_dynamic(
            self, 
            {nn.Linear}, 
            dtype=torch.qint8
        )
        
        # Then, fuse operations for better performance
        self = torch.jit.script(self)
        
        return self
```

#### Mixed-Precision Training

Using different precisions for different parts of the model:

```python
from torch.cuda.amp import autocast, GradScaler

def train_with_mixed_precision(model, train_loader, num_epochs=10):
    """
    Training with Automatic Mixed Precision for better performance
    """
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)
    scaler = GradScaler()
    criterion = nn.CrossEntropyLoss()
    
    model.train()
    for epoch in range(num_epochs):
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            
            # Forward pass with autocast
            with autocast():
                output = model(data)
                loss = criterion(output, target)
            
            # Backward pass with gradient scaling
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            if batch_idx % 100 == 0:
                print(f'Epoch {epoch}, Loss: {loss.item():.4f}')
```

## Part II: Knowledge Distillation - Learning from Teachers

### The Intuition Behind Distillation

Knowledge distillation is like having an experienced mentor teach a junior developer. The mentor (teacher model) has years of experience and deep understanding, while the junior (student model) is eager to learn but needs guidance to develop similar intuition quickly.

In ML terms, we train a smaller, faster "student" model to mimic the behavior of a larger, more accurate "teacher" model.

### Types of Knowledge Distillation

#### 1. Response-Based Distillation

The student learns from the teacher's final outputs:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class DistillationLoss(nn.Module):
    """
    Combined loss for knowledge distillation
    """
    def __init__(self, alpha=0.3, temperature=4.0):
        super().__init__()
        self.alpha = alpha  # Weight for distillation loss
        self.temperature = temperature  # Temperature for softmax
        self.kl_div = nn.KLDivLoss(reduction='batchmean')
        self.ce_loss = nn.CrossEntropyLoss()
        
    def forward(self, student_logits, teacher_logits, target):
        # Distillation loss - student learns from teacher's soft predictions
        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)
        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)
        distillation_loss = self.kl_div(student_log_probs, teacher_probs)
        
        # Standard classification loss - student learns from true labels
        classification_loss = self.ce_loss(student_logits, target)
        
        # Combined loss
        total_loss = (
            self.alpha * distillation_loss * (self.temperature ** 2) +
            (1 - self.alpha) * classification_loss
        )
        
        return total_loss, distillation_loss, classification_loss

def train_student_model(teacher_model, student_model, train_loader, num_epochs=20):
    """
    Train student model using knowledge distillation
    """
    teacher_model.eval()  # Teacher is frozen
    student_model.train()
    
    optimizer = torch.optim.AdamW(student_model.parameters(), lr=1e-3)
    distillation_criterion = DistillationLoss(alpha=0.7, temperature=4.0)
    
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            
            # Get teacher predictions (no gradients needed)
            with torch.no_grad():
                teacher_logits = teacher_model(data)
            
            # Get student predictions
            student_logits = student_model(data)
            
            # Calculate combined loss
            total_loss, dist_loss, class_loss = distillation_criterion(
                student_logits, teacher_logits, target
            )
            
            # Backward pass
            total_loss.backward()
            optimizer.step()
            
            epoch_loss += total_loss.item()
            
            if batch_idx % 100 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}')
                print(f'  Total Loss: {total_loss.item():.4f}')
                print(f'  Distillation Loss: {dist_loss.item():.4f}')
                print(f'  Classification Loss: {class_loss.item():.4f}')
        
        print(f'Epoch {epoch} Average Loss: {epoch_loss/len(train_loader):.4f}')
```

#### 2. Feature-Based Distillation

The student learns from intermediate representations of the teacher:

```python
class FeatureDistillationLoss(nn.Module):
    """
    Distillation loss that includes intermediate feature matching
    """
    def __init__(self, alpha=0.3, beta=0.3, temperature=4.0):
        super().__init__()
        self.alpha = alpha  # Weight for output distillation
        self.beta = beta    # Weight for feature distillation
        self.temperature = temperature
        
        self.kl_div = nn.KLDivLoss(reduction='batchmean')
        self.ce_loss = nn.CrossEntropyLoss()
        self.mse_loss = nn.MSELoss()
        
    def forward(self, student_outputs, teacher_outputs, target):
        student_logits, student_features = student_outputs
        teacher_logits, teacher_features = teacher_outputs
        
        # Output distillation loss
        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)
        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)
        output_dist_loss = self.kl_div(student_log_probs, teacher_probs)
        
        # Feature distillation loss
        feature_dist_loss = 0
        for s_feat, t_feat in zip(student_features, teacher_features):
            # Align dimensions if necessary
            if s_feat.shape != t_feat.shape:
                s_feat = F.adaptive_avg_pool2d(s_feat, t_feat.shape[-2:])
            feature_dist_loss += self.mse_loss(s_feat, t_feat.detach())
        
        # Classification loss
        classification_loss = self.ce_loss(student_logits, target)
        
        # Combined loss
        total_loss = (
            self.alpha * output_dist_loss * (self.temperature ** 2) +
            self.beta * feature_dist_loss +
            (1 - self.alpha - self.beta) * classification_loss
        )
        
        return total_loss

class TeacherStudentPair(nn.Module):
    """
    Wrapper for teacher-student training with feature extraction
    """
    def __init__(self, teacher_model, student_model):
        super().__init__()
        self.teacher = teacher_model
        self.student = student_model
        
        # Hooks to capture intermediate features
        self.teacher_features = []
        self.student_features = []
        
        self._register_hooks()
        
    def _register_hooks(self):
        """Register forward hooks to capture intermediate features"""
        def teacher_hook(module, input, output):
            self.teacher_features.append(output)
            
        def student_hook(module, input, output):
            self.student_features.append(output)
        
        # Register hooks on specific layers (e.g., after each transformer block)
        for name, module in self.teacher.named_modules():
            if 'layer' in name and 'attention' in name:
                module.register_forward_hook(teacher_hook)
                
        for name, module in self.student.named_modules():
            if 'layer' in name and 'attention' in name:
                module.register_forward_hook(student_hook)
    
    def forward(self, x):
        # Clear previous features
        self.teacher_features.clear()
        self.student_features.clear()
        
        # Forward pass through both models
        with torch.no_grad():
            teacher_output = self.teacher(x)
        
        student_output = self.student(x)
        
        return (student_output, self.student_features), (teacher_output, self.teacher_features)
```

#### 3. Progressive Knowledge Distillation

For very large models, we can use a series of intermediate teachers:

```python
def progressive_distillation(large_teacher, medium_student, small_student, train_loader):
    """
    Progressive distillation: Large -> Medium -> Small
    """
    print("Stage 1: Training medium student from large teacher...")
    
    # First stage: Large teacher -> Medium student
    trained_medium = train_student_model(
        teacher_model=large_teacher,
        student_model=medium_student,
        train_loader=train_loader,
        num_epochs=15
    )
    
    print("Stage 2: Training small student from medium teacher...")
    
    # Second stage: Medium teacher -> Small student
    trained_small = train_student_model(
        teacher_model=trained_medium,
        student_model=small_student,
        train_loader=train_loader,
        num_epochs=15
    )
    
    return trained_medium, trained_small
```

## Real-World Implementation: Optimizing a Recommendation System

Let me share how we applied these techniques at Mediacorp for our video recommendation system:

```python
class ProductionOptimizedRecommender:
    """
    Real-world implementation combining quantization and distillation
    """
    def __init__(self, large_model_path):
        # Load the large teacher model
        self.teacher_model = torch.load(large_model_path)
        self.teacher_model.eval()
        
        # Define smaller student architecture
        self.student_model = self._create_student_architecture()
        
        # Optimization pipeline
        self.optimization_pipeline = [
            self._knowledge_distillation,
            self._quantization,
            self._model_pruning,
            self._optimization_verification
        ]
    
    def _create_student_architecture(self):
        """Create a more efficient student model"""
        return nn.Sequential(
            nn.Embedding(100000, 128),  # Smaller embedding dim
            nn.TransformerEncoder(
                nn.TransformerEncoderLayer(
                    d_model=128,  # Reduced from 512
                    nhead=4,      # Reduced from 8
                    dim_feedforward=256,  # Reduced from 2048
                    dropout=0.1
                ),
                num_layers=3  # Reduced from 6
            ),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
    
    def _knowledge_distillation(self):
        """Apply knowledge distillation"""
        print("Applying knowledge distillation...")
        
        # Custom loss for ranking tasks
        class RankingDistillationLoss(nn.Module):
            def __init__(self, temperature=3.0):
                super().__init__()
                self.temperature = temperature
                self.mse_loss = nn.MSELoss()
                
            def forward(self, student_scores, teacher_scores, targets):
                # Distillation loss for ranking scores
                soft_teacher = F.softmax(teacher_scores / self.temperature, dim=1)
                soft_student = F.log_softmax(student_scores / self.temperature, dim=1)
                distillation_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean')
                
                # Direct score regression
                score_loss = self.mse_loss(student_scores, teacher_scores.detach())
                
                # Ranking loss
                ranking_loss = F.binary_cross_entropy_with_logits(student_scores, targets)
                
                return 0.4 * distillation_loss + 0.3 * score_loss + 0.3 * ranking_loss
        
        # Training implementation would go here
        pass
    
    def _quantization(self):
        """Apply quantization to the distilled model"""
        print("Applying quantization...")
        
        self.student_model = torch.quantization.quantize_dynamic(
            self.student_model,
            {nn.Linear, nn.Embedding},
            dtype=torch.qint8
        )
    
    def _model_pruning(self):
        """Apply structured pruning for additional compression"""
        print("Applying model pruning...")
        
        # This is a simplified example - real pruning is more complex
        for name, module in self.student_model.named_modules():
            if isinstance(module, nn.Linear):
                # Remove channels with smallest L1 norm
                weights = module.weight.data
                l1_norm = torch.sum(torch.abs(weights), dim=1)
                _, indices = torch.topk(l1_norm, int(0.8 * len(l1_norm)))
                
                # Keep only top 80% of channels
                module.weight.data = weights[indices]
                if module.bias is not None:
                    module.bias.data = module.bias.data[indices]
    
    def _optimization_verification(self):
        """Verify optimization results"""
        print("Verifying optimization results...")
        
        # Performance benchmarking
        dummy_input = torch.randint(0, 1000, (32, 100))
        
        # Measure original model
        start_time = time.time()
        with torch.no_grad():
            teacher_output = self.teacher_model(dummy_input)
        teacher_time = time.time() - start_time
        
        # Measure optimized model
        start_time = time.time()
        with torch.no_grad():
            student_output = self.student_model(dummy_input)
        student_time = time.time() - start_time
        
        # Calculate metrics
        speedup = teacher_time / student_time
        size_reduction = self._calculate_model_size_reduction()
        
        print(f"Speedup: {speedup:.2f}x")
        print(f"Size reduction: {size_reduction:.2f}x")
        
        return {
            'speedup': speedup,
            'size_reduction': size_reduction,
            'teacher_latency': teacher_time,
            'student_latency': student_time
        }
    
    def optimize(self):
        """Run the complete optimization pipeline"""
        results = {}
        for step in self.optimization_pipeline:
            step_result = step()
            if step_result:
                results.update(step_result)
        
        return results
```

## Best Practices and Gotchas

### When to Use What

**Use Quantization when:**
- You have limited memory constraints
- Inference latency is critical
- You can afford some accuracy loss (usually 1-3%)
- Your model has many linear/conv layers

**Use Knowledge Distillation when:**
- You need to maintain high accuracy
- You have access to unlabeled data
- Model interpretability matters
- You're working with ensemble models

**Combine Both when:**
- Maximum compression is needed
- You have computational budget for training
- Production constraints are severe

### Common Pitfalls and How to Avoid Them

```python
class OptimizationBestPractices:
    """
    Common mistakes and how to avoid them
    """
    
    @staticmethod
    def calibration_data_selection():
        """
        MISTAKE: Using random data for quantization calibration
        SOLUTION: Use representative production data
        """
        # Bad
        calibration_data = torch.randn(100, 512)
        
        # Good - use actual data distribution
        calibration_data = load_production_sample_data()
        
        return calibration_data
    
    @staticmethod
    def temperature_tuning():
        """
        MISTAKE: Using fixed temperature for all tasks
        SOLUTION: Tune temperature based on task and data
        """
        def find_optimal_temperature(teacher_outputs, validation_targets):
            best_temp = 1.0
            best_nll = float('inf')
            
            for temp in [1, 2, 3, 4, 5, 8, 10]:
                soft_outputs = F.log_softmax(teacher_outputs / temp, dim=1)
                nll = F.nll_loss(soft_outputs, validation_targets)
                
                if nll < best_nll:
                    best_nll = nll
                    best_temp = temp
            
            return best_temp
    
    @staticmethod
    def gradual_optimization():
        """
        MISTAKE: Applying all optimizations at once
        SOLUTION: Gradual optimization with validation at each step
        """
        def optimize_gradually(model, validation_loader):
            original_accuracy = evaluate_model(model, validation_loader)
            print(f"Original accuracy: {original_accuracy:.4f}")
            
            # Step 1: Knowledge distillation
            model = apply_distillation(model)
            distilled_accuracy = evaluate_model(model, validation_loader)
            print(f"After distillation: {distilled_accuracy:.4f}")
            
            if distilled_accuracy < original_accuracy * 0.95:
                print("Distillation caused too much accuracy loss, reverting...")
                return model
            
            # Step 2: Quantization
            model = apply_quantization(model)
            quantized_accuracy = evaluate_model(model, validation_loader)
            print(f"After quantization: {quantized_accuracy:.4f}")
            
            if quantized_accuracy < original_accuracy * 0.90:
                print("Quantization caused too much accuracy loss, using different approach...")
                # Try different quantization strategy
                pass
            
            return model
```

## Advanced Topics and Future Directions

### Emerging Techniques

#### Neural Architecture Search for Compressed Models
```python
class NAS_Distillation:
    """
    Using NAS to find optimal student architectures
    """
    def search_student_architecture(self, teacher_model, search_space):
        # Implementation would involve evolutionary algorithms
        # or differentiable NAS to find optimal student
        pass
```

#### Attention Transfer
```python
def attention_transfer_loss(student_attention, teacher_attention):
    """
    Transfer attention maps from teacher to student
    """
    # Normalize attention maps
    student_att = F.normalize(student_attention.view(-1), p=2, dim=0)
    teacher_att = F.normalize(teacher_attention.view(-1), p=2, dim=0)
    
    # Attention transfer loss
    return F.mse_loss(student_att, teacher_att)
```

### Production Deployment Considerations

```python
class ProductionDeployment:
    """
    Real considerations for production deployment
    """
    
    def __init__(self, optimized_model):
        self.model = optimized_model
        
    def export_for_inference(self):
        """
        Export model in production-ready format
        """
        # TorchScript for PyTorch models
        traced_model = torch.jit.trace(self.model, example_input)
        traced_model.save("model.pt")
        
        # ONNX for cross-platform deployment
        torch.onnx.export(
            self.model, 
            example_input, 
            "model.onnx",
            export_params=True,
            opset_version=11,
            do_constant_folding=True
        )
        
        return traced_model
    
    def benchmark_production_metrics(self):
        """
        Comprehensive production benchmarking
        """
        metrics = {
            'latency_p50': self._measure_latency_percentile(50),
            'latency_p95': self._measure_latency_percentile(95),
            'latency_p99': self._measure_latency_percentile(99),
            'throughput': self._measure_throughput(),
            'memory_usage': self._measure_memory_usage(),
            'cpu_usage': self._measure_cpu_usage(),
            'accuracy': self._measure_accuracy()
        }
        
        return metrics
```

## Conclusion: The Art and Science of Model Optimization

Model optimization is both an art and a science. The science lies in understanding the mathematical foundations of quantization and distillation. The art lies in knowing when and how to apply these techniques to achieve the right balance between performance, accuracy, and resource constraints.

In my experience building production ML systems, I've learned that:

1. **Start with your constraints**: Understand your latency, memory, and accuracy requirements before choosing optimization techniques.

2. **Measure everything**: Always benchmark before and after optimization. What gets measured gets managed.

3. **Gradual optimization works best**: Apply techniques incrementally and validate at each step.

4. **Production data matters**: Use representative data for calibration and validation.

5. **Monitor continuously**: Model performance can degrade in production due to data drift, so continuous monitoring is crucial.

The techniques covered in this post - from basic quantization to advanced knowledge distillation - are powerful tools in the ML engineer's toolkit. But remember, they're means to an end: building ML systems that deliver real value to users while operating efficiently at scale.

As we move toward even larger models and more constrained deployment environments, these optimization techniques will only become more important. The future belongs to engineers who can bridge the gap between state-of-the-art research and production-ready systems.

---

*Have you implemented quantization or knowledge distillation in your projects? What challenges did you face, and what worked well? I'd love to hear about your experiences in the comments below.*

## References and Further Reading

1. Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network.
2. Jacob, B., et al. (2018). Quantization and training of neural networks for efficient integer-arithmetic-only inference.
3. Romero, A., et al. (2014). FitNets: Hints for thin deep nets.
4. Wu, J., Leng, C., Wang, Y., Hu, Q., & Cheng, J. (2016). Quantized convolutional neural networks for mobile devices.
5. PyTorch Quantization Documentation
6. TensorFlow Model Optimization Toolkit Documentation