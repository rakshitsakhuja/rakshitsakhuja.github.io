---
title: "Model Optimization Techniques: A Deep Dive into Quantization and Knowledge Distillation"
author: "Rakshit Sakhuja"
date: "2025-08-08"
categories: [Deep Learning, Model Optimization, MLOps, Production ML]
image: "quantization_distillation.png"
description: "Technical implementation guide to model compression techniques - quantization and distillation methods with code examples and performance benchmarks."
---

# Model Optimization: Quantization and Knowledge Distillation

Model compression is essential for deploying large neural networks in production. This post covers two core techniques: **quantization** (reducing numerical precision) and **knowledge distillation** (training smaller models to mimic larger ones).

## Performance Impact Overview

Here are typical optimization results from a 175M parameter transformer model:

| Technique | Model Size | Inference Time | Memory Usage | Accuracy Drop |
|-----------|------------|----------------|--------------|---------------|
| Baseline FP32 | 700MB | 45ms | 2.1GB | - |
| FP16 Mixed Precision | 350MB | 28ms | 1.2GB | <0.1% |
| INT8 Quantization | 175MB | 18ms | 0.8GB | 0.5-2% |
| Knowledge Distillation | 87MB | 12ms | 0.4GB | 2-5% |
| Combined | 44MB | 8ms | 0.2GB | 3-7% |

## Part I: Quantization - Precision vs Performance

### What is Quantization?

Quantization reduces numerical precision from FP32 to lower bit formats (FP16, INT8, INT4). Here's the mathematical transformation:

**FP32 → INT8 Quantization:**
```
quantized_value = round((fp32_value - zero_point) * scale)
dequantized_value = quantized_value / scale + zero_point
```

**Example Weight Transformation:**
```python
# Original FP32 weights
original_weights = [0.342, -0.891, 1.234, -2.156]

# Quantization parameters
scale = 0.0157  # (max_val - min_val) / (2^8 - 1)
zero_point = 128

# Quantized INT8 weights
quantized = [150, 71, 206, 28]  # 8-bit integers

# Memory reduction: 32 bits → 8 bits (4x compression)
```

**Precision Impact:**
- FP32: 32 bits, range ±3.4×10³⁸, precision ~7 decimals
- FP16: 16 bits, range ±65,504, precision ~3 decimals  
- INT8: 8 bits, range -128 to 127, discrete values only

### Types of Quantization

#### 1. Post-Training Quantization (PTQ)

Quantize pre-trained models without retraining:

```python
import torch
import torch.quantization as quant

def quantize_model_ptq(model, calibration_loader):
    model.eval()
    model.qconfig = quant.get_default_qconfig('fbgemm')
    model_prepared = quant.prepare(model)
    
    # Calibration pass
    with torch.no_grad():
        for batch in calibration_loader:
            model_prepared(batch)
    
    return quant.convert(model_prepared)

# Example results
original_model = MyTransformer(d_model=512, layers=6)
quantized_model = quantize_model_ptq(original_model, calibration_loader)

# Input: torch.randn(32, 128, 512)  # batch_size=32, seq_len=128, d_model=512
# Original output: torch.Size([32, 128, 512]), dtype=torch.float32
# Quantized output: torch.Size([32, 128, 512]), dtype=torch.float32 (internally INT8)

# Benchmark results:
# Original: 234MB, 67ms inference
# Quantized: 58MB, 23ms inference  
# Compression: 4.0x, Speedup: 2.9x
```

#### 2. Quantization-Aware Training (QAT)

QAT simulates quantization effects during training, allowing the model to adapt to reduced precision.

```python
import torch.nn as nn
import torch.quantization as quant

class QuantizedTransformerBlock(nn.Module):
    """
    A transformer block prepared for quantization-aware training
    """
    def __init__(self, d_model, nhead, dim_feedforward):
        super().__init__()
        self.attention = nn.MultiheadAttention(d_model, nhead)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        self.feedforward = nn.Sequential(
            nn.Linear(d_model, dim_feedforward),
            nn.ReLU(),
            nn.Linear(dim_feedforward, d_model)
        )
        
        # Add quantization stubs
        self.quant = quant.QuantStub()
        self.dequant = quant.DeQuantStub()
        
    def forward(self, x):
        x = self.quant(x)
        
        # Attention block with residual connection
        attn_out, _ = self.attention(x, x, x)
        x = self.norm1(x + attn_out)
        
        # Feedforward block with residual connection
        ff_out = self.feedforward(x)
        x = self.norm2(x + ff_out)
        
        x = self.dequant(x)
        return x

def train_with_qat(model, train_loader, num_epochs=10):
    """
    Training loop for quantization-aware training
    """
    # Configure quantization settings
    model.qconfig = quant.get_default_qat_qconfig('fbgemm')
    
    # Prepare model for QAT
    model_prepared = quant.prepare_qat(model, inplace=False)
    
    # Training loop
    optimizer = torch.optim.AdamW(model_prepared.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss()
    
    model_prepared.train()
    for epoch in range(num_epochs):
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            output = model_prepared(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            if batch_idx % 100 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')
    
    # Convert to quantized model for deployment
    model_prepared.eval()
    quantized_model = quant.convert(model_prepared, inplace=False)
    
    return quantized_model
```

### Advanced Quantization Techniques

#### Dynamic Quantization

Perfect for models with varying input sizes, like NLP applications:

```python
def apply_dynamic_quantization(model):
    """
    Apply dynamic quantization - weights are quantized, 
    activations are quantized dynamically during inference
    """
    quantized_model = torch.quantization.quantize_dynamic(
        model, 
        {nn.Linear, nn.LSTM, nn.GRU}, 
        dtype=torch.qint8
    )
    return quantized_model

# Example usage with a BERT-style model
class OptimizedBERT(nn.Module):
    def __init__(self, config):
        super().__init__()
        # Your BERT implementation here
        pass
    
    def optimize_for_production(self):
        """
        Apply multiple optimization techniques
        """
        # First, apply dynamic quantization
        self = torch.quantization.quantize_dynamic(
            self, 
            {nn.Linear}, 
            dtype=torch.qint8
        )
        
        # Then, fuse operations for better performance
        self = torch.jit.script(self)
        
        return self
```

#### Mixed-Precision Training

Using different precisions for different parts of the model:

```python
from torch.cuda.amp import autocast, GradScaler

def train_with_mixed_precision(model, train_loader, num_epochs=10):
    """
    Training with Automatic Mixed Precision for better performance
    """
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)
    scaler = GradScaler()
    criterion = nn.CrossEntropyLoss()
    
    model.train()
    for epoch in range(num_epochs):
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            
            # Forward pass with autocast
            with autocast():
                output = model(data)
                loss = criterion(output, target)
            
            # Backward pass with gradient scaling
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            if batch_idx % 100 == 0:
                print(f'Epoch {epoch}, Loss: {loss.item():.4f}')
```

## Part II: Knowledge Distillation - Learning from Teachers

### Knowledge Distillation Mechanics

Knowledge distillation trains a small "student" model to mimic a large "teacher" model using soft targets instead of hard labels.

**Mathematical Foundation:**
```
L_distillation = α * KL_div(softmax(z_s/T), softmax(z_t/T)) + 
                 (1-α) * CrossEntropy(z_s, y_true)
```

Where:
- z_s: student logits, z_t: teacher logits  
- T: temperature (typically 3-5)
- α: distillation weight (typically 0.7-0.9)

**Concrete Example:**
```python
# Teacher output (large model): [2.1, 0.8, -1.2]
# Hard labels: [1, 0, 0] (one-hot)
# Soft labels (T=3): [0.52, 0.35, 0.13] (temperature-scaled softmax)

# Student learns from both:
# - Soft targets: richer information about similarities between classes
# - Hard targets: ground truth for accuracy
```

### Types of Knowledge Distillation

#### 1. Response-Based Distillation

The student learns from the teacher's final outputs:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class DistillationLoss(nn.Module):
    """
    Combined loss for knowledge distillation
    """
    def __init__(self, alpha=0.3, temperature=4.0):
        super().__init__()
        self.alpha = alpha  # Weight for distillation loss
        self.temperature = temperature  # Temperature for softmax
        self.kl_div = nn.KLDivLoss(reduction='batchmean')
        self.ce_loss = nn.CrossEntropyLoss()
        
    def forward(self, student_logits, teacher_logits, target):
        # Distillation loss - student learns from teacher's soft predictions
        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)
        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)
        distillation_loss = self.kl_div(student_log_probs, teacher_probs)
        
        # Standard classification loss - student learns from true labels
        classification_loss = self.ce_loss(student_logits, target)
        
        # Combined loss
        total_loss = (
            self.alpha * distillation_loss * (self.temperature ** 2) +
            (1 - self.alpha) * classification_loss
        )
        
        return total_loss, distillation_loss, classification_loss

def train_student_model(teacher_model, student_model, train_loader, num_epochs=20):
    """
    Train student model using knowledge distillation
    """
    teacher_model.eval()  # Teacher is frozen
    student_model.train()
    
    optimizer = torch.optim.AdamW(student_model.parameters(), lr=1e-3)
    distillation_criterion = DistillationLoss(alpha=0.7, temperature=4.0)
    
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            
            # Get teacher predictions (no gradients needed)
            with torch.no_grad():
                teacher_logits = teacher_model(data)
            
            # Get student predictions
            student_logits = student_model(data)
            
            # Calculate combined loss
            total_loss, dist_loss, class_loss = distillation_criterion(
                student_logits, teacher_logits, target
            )
            
            # Backward pass
            total_loss.backward()
            optimizer.step()
            
            epoch_loss += total_loss.item()
            
            if batch_idx % 100 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}')
                print(f'  Total Loss: {total_loss.item():.4f}')
                print(f'  Distillation Loss: {dist_loss.item():.4f}')
                print(f'  Classification Loss: {class_loss.item():.4f}')
        
        print(f'Epoch {epoch} Average Loss: {epoch_loss/len(train_loader):.4f}')
```

#### 2. Feature-Based Distillation

The student learns from intermediate representations of the teacher:

```python
class FeatureDistillationLoss(nn.Module):
    """
    Distillation loss that includes intermediate feature matching
    """
    def __init__(self, alpha=0.3, beta=0.3, temperature=4.0):
        super().__init__()
        self.alpha = alpha  # Weight for output distillation
        self.beta = beta    # Weight for feature distillation
        self.temperature = temperature
        
        self.kl_div = nn.KLDivLoss(reduction='batchmean')
        self.ce_loss = nn.CrossEntropyLoss()
        self.mse_loss = nn.MSELoss()
        
    def forward(self, student_outputs, teacher_outputs, target):
        student_logits, student_features = student_outputs
        teacher_logits, teacher_features = teacher_outputs
        
        # Output distillation loss
        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)
        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)
        output_dist_loss = self.kl_div(student_log_probs, teacher_probs)
        
        # Feature distillation loss
        feature_dist_loss = 0
        for s_feat, t_feat in zip(student_features, teacher_features):
            # Align dimensions if necessary
            if s_feat.shape != t_feat.shape:
                s_feat = F.adaptive_avg_pool2d(s_feat, t_feat.shape[-2:])
            feature_dist_loss += self.mse_loss(s_feat, t_feat.detach())
        
        # Classification loss
        classification_loss = self.ce_loss(student_logits, target)
        
        # Combined loss
        total_loss = (
            self.alpha * output_dist_loss * (self.temperature ** 2) +
            self.beta * feature_dist_loss +
            (1 - self.alpha - self.beta) * classification_loss
        )
        
        return total_loss

class TeacherStudentPair(nn.Module):
    """
    Wrapper for teacher-student training with feature extraction
    """
    def __init__(self, teacher_model, student_model):
        super().__init__()
        self.teacher = teacher_model
        self.student = student_model
        
        # Hooks to capture intermediate features
        self.teacher_features = []
        self.student_features = []
        
        self._register_hooks()
        
    def _register_hooks(self):
        """Register forward hooks to capture intermediate features"""
        def teacher_hook(module, input, output):
            self.teacher_features.append(output)
            
        def student_hook(module, input, output):
            self.student_features.append(output)
        
        # Register hooks on specific layers (e.g., after each transformer block)
        for name, module in self.teacher.named_modules():
            if 'layer' in name and 'attention' in name:
                module.register_forward_hook(teacher_hook)
                
        for name, module in self.student.named_modules():
            if 'layer' in name and 'attention' in name:
                module.register_forward_hook(student_hook)
    
    def forward(self, x):
        # Clear previous features
        self.teacher_features.clear()
        self.student_features.clear()
        
        # Forward pass through both models
        with torch.no_grad():
            teacher_output = self.teacher(x)
        
        student_output = self.student(x)
        
        return (student_output, self.student_features), (teacher_output, self.teacher_features)
```

#### 3. Progressive Knowledge Distillation

For very large models, we can use a series of intermediate teachers:

```python
def progressive_distillation(large_teacher, medium_student, small_student, train_loader):
    """
    Progressive distillation: Large -> Medium -> Small
    """
    print("Stage 1: Training medium student from large teacher...")
    
    # First stage: Large teacher -> Medium student
    trained_medium = train_student_model(
        teacher_model=large_teacher,
        student_model=medium_student,
        train_loader=train_loader,
        num_epochs=15
    )
    
    print("Stage 2: Training small student from medium teacher...")
    
    # Second stage: Medium teacher -> Small student
    trained_small = train_student_model(
        teacher_model=trained_medium,
        student_model=small_student,
        train_loader=train_loader,
        num_epochs=15
    )
    
    return trained_medium, trained_small
```

## Real-World Implementation: Optimizing a Recommendation System

Let me share how we applied these techniques in production for a video recommendation system:

```python
class ProductionOptimizedRecommender:
    """
    Real-world implementation combining quantization and distillation
    """
    def __init__(self, large_model_path):
        # Load the large teacher model
        self.teacher_model = torch.load(large_model_path)
        self.teacher_model.eval()
        
        # Define smaller student architecture
        self.student_model = self._create_student_architecture()
        
        # Optimization pipeline
        self.optimization_pipeline = [
            self._knowledge_distillation,
            self._quantization,
            self._model_pruning,
            self._optimization_verification
        ]
    
    def _create_student_architecture(self):
        """Create a more efficient student model"""
        return nn.Sequential(
            nn.Embedding(100000, 128),  # Smaller embedding dim
            nn.TransformerEncoder(
                nn.TransformerEncoderLayer(
                    d_model=128,  # Reduced from 512
                    nhead=4,      # Reduced from 8
                    dim_feedforward=256,  # Reduced from 2048
                    dropout=0.1
                ),
                num_layers=3  # Reduced from 6
            ),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
    
    def _knowledge_distillation(self):
        """Apply knowledge distillation"""
        print("Applying knowledge distillation...")
        
        # Custom loss for ranking tasks
        class RankingDistillationLoss(nn.Module):
            def __init__(self, temperature=3.0):
                super().__init__()
                self.temperature = temperature
                self.mse_loss = nn.MSELoss()
                
            def forward(self, student_scores, teacher_scores, targets):
                # Distillation loss for ranking scores
                soft_teacher = F.softmax(teacher_scores / self.temperature, dim=1)
                soft_student = F.log_softmax(student_scores / self.temperature, dim=1)
                distillation_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean')
                
                # Direct score regression
                score_loss = self.mse_loss(student_scores, teacher_scores.detach())
                
                # Ranking loss
                ranking_loss = F.binary_cross_entropy_with_logits(student_scores, targets)
                
                return 0.4 * distillation_loss + 0.3 * score_loss + 0.3 * ranking_loss
        
        # Training implementation would go here
        pass
    
    def _quantization(self):
        """Apply quantization to the distilled model"""
        print("Applying quantization...")
        
        self.student_model = torch.quantization.quantize_dynamic(
            self.student_model,
            {nn.Linear, nn.Embedding},
            dtype=torch.qint8
        )
    
    def _model_pruning(self):
        """Apply structured pruning for additional compression"""
        print("Applying model pruning...")
        
        # This is a simplified example - real pruning is more complex
        for name, module in self.student_model.named_modules():
            if isinstance(module, nn.Linear):
                # Remove channels with smallest L1 norm
                weights = module.weight.data
                l1_norm = torch.sum(torch.abs(weights), dim=1)
                _, indices = torch.topk(l1_norm, int(0.8 * len(l1_norm)))
                
                # Keep only top 80% of channels
                module.weight.data = weights[indices]
                if module.bias is not None:
                    module.bias.data = module.bias.data[indices]
    
    def _optimization_verification(self):
        """Verify optimization results"""
        print("Verifying optimization results...")
        
        # Performance benchmarking
        dummy_input = torch.randint(0, 1000, (32, 100))
        
        # Measure original model
        start_time = time.time()
        with torch.no_grad():
            teacher_output = self.teacher_model(dummy_input)
        teacher_time = time.time() - start_time
        
        # Measure optimized model
        start_time = time.time()
        with torch.no_grad():
            student_output = self.student_model(dummy_input)
        student_time = time.time() - start_time
        
        # Calculate metrics
        speedup = teacher_time / student_time
        size_reduction = self._calculate_model_size_reduction()
        
        print(f"Speedup: {speedup:.2f}x")
        print(f"Size reduction: {size_reduction:.2f}x")
        
        return {
            'speedup': speedup,
            'size_reduction': size_reduction,
            'teacher_latency': teacher_time,
            'student_latency': student_time
        }
    
    def optimize(self):
        """Run the complete optimization pipeline"""
        results = {}
        for step in self.optimization_pipeline:
            step_result = step()
            if step_result:
                results.update(step_result)
        
        return results
```

## Best Practices and Gotchas

### When to Use What

**Use Quantization when:**
- You have limited memory constraints
- Inference latency is critical
- You can afford some accuracy loss (usually 1-3%)
- Your model has many linear/conv layers

**Use Knowledge Distillation when:**
- You need to maintain high accuracy
- You have access to unlabeled data
- Model interpretability matters
- You're working with ensemble models

**Combine Both when:**
- Maximum compression is needed
- You have computational budget for training
- Production constraints are severe

### Common Pitfalls and How to Avoid Them

```python
class OptimizationBestPractices:
    """
    Common mistakes and how to avoid them
    """
    
    @staticmethod
    def calibration_data_selection():
        """
        MISTAKE: Using random data for quantization calibration
        SOLUTION: Use representative production data
        """
        # Bad
        calibration_data = torch.randn(100, 512)
        
        # Good - use actual data distribution
        calibration_data = load_production_sample_data()
        
        return calibration_data
    
    @staticmethod
    def temperature_tuning():
        """
        MISTAKE: Using fixed temperature for all tasks
        SOLUTION: Tune temperature based on task and data
        """
        def find_optimal_temperature(teacher_outputs, validation_targets):
            best_temp = 1.0
            best_nll = float('inf')
            
            for temp in [1, 2, 3, 4, 5, 8, 10]:
                soft_outputs = F.log_softmax(teacher_outputs / temp, dim=1)
                nll = F.nll_loss(soft_outputs, validation_targets)
                
                if nll < best_nll:
                    best_nll = nll
                    best_temp = temp
            
            return best_temp
    
    @staticmethod
    def gradual_optimization():
        """
        MISTAKE: Applying all optimizations at once
        SOLUTION: Gradual optimization with validation at each step
        """
        def optimize_gradually(model, validation_loader):
            original_accuracy = evaluate_model(model, validation_loader)
            print(f"Original accuracy: {original_accuracy:.4f}")
            
            # Step 1: Knowledge distillation
            model = apply_distillation(model)
            distilled_accuracy = evaluate_model(model, validation_loader)
            print(f"After distillation: {distilled_accuracy:.4f}")
            
            if distilled_accuracy < original_accuracy * 0.95:
                print("Distillation caused too much accuracy loss, reverting...")
                return model
            
            # Step 2: Quantization
            model = apply_quantization(model)
            quantized_accuracy = evaluate_model(model, validation_loader)
            print(f"After quantization: {quantized_accuracy:.4f}")
            
            if quantized_accuracy < original_accuracy * 0.90:
                print("Quantization caused too much accuracy loss, using different approach...")
                # Try different quantization strategy
                pass
            
            return model
```

## Advanced Topics and Future Directions

### Emerging Techniques

#### Neural Architecture Search for Compressed Models
```python
class NAS_Distillation:
    """
    Using NAS to find optimal student architectures
    """
    def search_student_architecture(self, teacher_model, search_space):
        # Implementation would involve evolutionary algorithms
        # or differentiable NAS to find optimal student
        pass
```

#### Attention Transfer
```python
def attention_transfer_loss(student_attention, teacher_attention):
    """
    Transfer attention maps from teacher to student
    """
    # Normalize attention maps
    student_att = F.normalize(student_attention.view(-1), p=2, dim=0)
    teacher_att = F.normalize(teacher_attention.view(-1), p=2, dim=0)
    
    # Attention transfer loss
    return F.mse_loss(student_att, teacher_att)
```

### Production Deployment Considerations

```python
class ProductionDeployment:
    """
    Real considerations for production deployment
    """
    
    def __init__(self, optimized_model):
        self.model = optimized_model
        
    def export_for_inference(self):
        """
        Export model in production-ready format
        """
        # TorchScript for PyTorch models
        traced_model = torch.jit.trace(self.model, example_input)
        traced_model.save("model.pt")
        
        # ONNX for cross-platform deployment
        torch.onnx.export(
            self.model, 
            example_input, 
            "model.onnx",
            export_params=True,
            opset_version=11,
            do_constant_folding=True
        )
        
        return traced_model
    
    def benchmark_production_metrics(self):
        """
        Comprehensive production benchmarking
        """
        metrics = {
            'latency_p50': self._measure_latency_percentile(50),
            'latency_p95': self._measure_latency_percentile(95),
            'latency_p99': self._measure_latency_percentile(99),
            'throughput': self._measure_throughput(),
            'memory_usage': self._measure_memory_usage(),
            'cpu_usage': self._measure_cpu_usage(),
            'accuracy': self._measure_accuracy()
        }
        
        return metrics
```

## Conclusion: The Art and Science of Model Optimization

Model optimization is both an art and a science. The science lies in understanding the mathematical foundations of quantization and distillation. The art lies in knowing when and how to apply these techniques to achieve the right balance between performance, accuracy, and resource constraints.

In my experience building production ML systems, I've learned that:

1. **Start with your constraints**: Understand your latency, memory, and accuracy requirements before choosing optimization techniques.

2. **Measure everything**: Always benchmark before and after optimization. What gets measured gets managed.

3. **Gradual optimization works best**: Apply techniques incrementally and validate at each step.

4. **Production data matters**: Use representative data for calibration and validation.

5. **Monitor continuously**: Model performance can degrade in production due to data drift, so continuous monitoring is crucial.

The techniques covered in this post - from basic quantization to advanced knowledge distillation - are powerful tools in the ML engineer's toolkit. But remember, they're means to an end: building ML systems that deliver real value to users while operating efficiently at scale.

As we move toward even larger models and more constrained deployment environments, these optimization techniques will only become more important. The future belongs to engineers who can bridge the gap between state-of-the-art research and production-ready systems.

---

## References and Further Reading

1. Hinton, G., Vinyals, O., & Dean, J. (2015). *Distilling the knowledge in a neural network*. arXiv preprint arXiv:1503.02531.

2. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., & Kalenichenko, D. (2018). *Quantization and training of neural networks for efficient integer-arithmetic-only inference*. Proceedings of the IEEE conference on computer vision and pattern recognition.

3. Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., & Bengio, Y. (2014). *FitNets: Hints for thin deep nets*. arXiv preprint arXiv:1412.6550.

4. Wu, J., Leng, C., Wang, Y., Hu, Q., & Cheng, J. (2016). *Quantized convolutional neural networks for mobile devices*. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.

5. Guo, Y., Yao, A., & Chen, Y. (2016). *Dynamic network surgery for efficient dnns*. Advances in neural information processing systems.

6. [PyTorch Quantization Documentation](https://pytorch.org/docs/stable/quantization.html)

7. [TensorFlow Model Optimization Toolkit](https://www.tensorflow.org/model_optimization)